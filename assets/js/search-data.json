{
  
    
        "post0": {
            "title": "Alternate AA index and Fourier Transform",
            "content": "Alternate AAindex - Hydrophobicity . In this post, we will be trying an alternative AAindex - Amino Acid Index - called Hydrophobicity index and also look into incorporating Fourier Transform into the proceedings. . The First Section will just be a repeat of the Data Engineering process already introduced in the previous sections of the blog, with the only edit being that we are using a different AAindex. So, please feel free to skip to the Linear Regression section. . import pandas as pd import numpy as np . h_index = pd.read_csv(&quot;hydrophobicity_index.csv&quot;) . h_index.iloc[:1] . A R N D C Q E G H I L K M F P S T W Y V . 0 0.61 | 0.6 | 0.06 | 0.46 | 1.07 | 0.0 | 0.47 | 0.07 | 0.61 | 2.22 | 1.53 | 1.15 | 1.18 | 2.02 | 1.95 | 0.05 | 0.05 | 2.65 | 1.88 | 1.32 | . h_dict = {} for i in h_index.columns: h_dict[i] = h_index[i][0] . Dictionary with the Hydrophobicity indexes . h_dict . {&#39;A&#39;: 0.61, &#39;R&#39;: 0.6, &#39;N&#39;: 0.06, &#39;D&#39;: 0.46, &#39;C&#39;: 1.07, &#39;Q&#39;: 0.0, &#39;E&#39;: 0.47, &#39;G&#39;: 0.07, &#39;H&#39;: 0.61, &#39;I&#39;: 2.22, &#39;L&#39;: 1.53, &#39;K&#39;: 1.15, &#39;M&#39;: 1.18, &#39;F&#39;: 2.02, &#39;P&#39;: 1.95, &#39;S&#39;: 0.05, &#39;T&#39;: 0.05, &#39;W&#39;: 2.65, &#39;Y&#39;: 1.88, &#39;V&#39;: 1.32} . train_data = pd.read_csv(&quot;ee_train.csv&quot;) test_data = pd.read_csv(&quot;ee_test.csv&quot;) print(train_data) print(test_data) . Type Sequence EE G 0 WT LARLTTMLC 4 -0.85 1 1 FARLTTMLC 12 -1.50 2 2 LNRLTTMLC 7 -1.17 3 3 LASLTTMLC 4 -0.85 4 4 LARYTTMLC 4 -0.85 5 5 LARLWTMLC 12 -1.50 6 6 LARLTVMLC 4 -0.85 7 7 LARLTTPLC 6 -1.08 8 8 LARLTTMYC 4 -0.85 9 9 LARLTTMLV 5 -0.97 Sequence G 0 FNSLTTMLC -1.68 1 LARLTTPYC -0.87 2 LARLWVMLC -1.68 3 FNSLTTPYC -1.84 4 FNSLTTMLV -1.67 5 FNSLWVMLC -2.19 6 FNSYTTMLC -1.93 7 LARLTTPYV -0.90 8 LARLWVPYC -1.30 9 LARYTTPYC -0.98 10 LARLWVMLV -1.73 11 LARYTTMLV -0.89 12 LARYWVMLC -1.88 13 FNSLTTPYV -1.92 14 FNSLWVPYC -2.15 15 FNSYTTPYC -1.96 16 FNSLWVMLV -2.41 17 FNSYTTMLV -1.85 18 FNSYWVMLC -2.37 19 LARLWVPYV -1.51 20 LARYTTPYV -0.92 21 LARYWVPYC -1.75 22 LARYWVMLV -1.74 23 FNSYTTPYV -2.57 24 FNSLWVPYV -2.09 25 FNSYWVPYC -2.32 26 FNSYWVMLV -2.73 27 FNSYWVPYV -2.87 . def convert_seq(x): list_seq = [] for i in x: list_seq.append(h_dict[i]) return list_seq . print(convert_seq(&#39;LARLTTMLC&#39;)) . [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07] . x_train = list(map(convert_seq,train_data[&quot;Sequence&quot;])) x_test = list(map(convert_seq,test_data[&quot;Sequence&quot;])) . x_train . [[1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [2.02, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.06, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.05, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.88, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 2.65, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 1.32, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.95, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.88, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.32]] . y_train = train_data[&quot;G&quot;] . y_test = test_data[&quot;G&quot;] . Simple Linear Regression . from sklearn.linear_model import LinearRegression reg = LinearRegression() . reg.fit(x_train,y_train) . LinearRegression() . prediction = reg.predict(x_test) . print(prediction) . [-1.82 -1.08 -1.5 -2.05 -1.94 -2.47 -1.82 -1.2 -1.73 -1.08 -1.62 -0.97 -1.5 -2.17 -2.7 -2.05 -2.59 -1.94 -2.47 -1.85 -1.2 -1.73 -1.62 -2.17 -2.82 -2.7 -2.59 -2.82] . R2 Score for Test Set . from sklearn.metrics import r2_score r2_score(y_test,prediction) . 0.730824152182818 . As promised, i did try the same simple linear regression model with another AAindex and we get results similar to the one we obtained with the EIIP AAindex and the One-Hot model. This trend continues throughout for the closed form linear regression model, and hence we will not be considering it for our task. . SVR . from sklearn.svm import SVR . The parameters we are considering for SVR . parameters = [&#39;linear&#39;,&#39;poly&#39;,&#39;rbf&#39;,&#39;sigmoid&#39;] . Default SVR . svr = SVR() svr.fit(x_train,y_train) print(svr.score(x_train,y_train)) . 0.6209572424941144 . svr.predict(x_test) . array([-1.17149025, -0.98769153, -1.30984363, -1.16435546, -1.17187482, -1.29976936, -1.15932717, -0.99132529, -1.28701036, -0.98135599, -1.3077824 , -0.95447213, -1.30433144, -1.16486616, -1.27907534, -1.15482163, -1.29788623, -1.15992676, -1.2949261 , -1.28535278, -0.98510175, -1.28256079, -1.30236765, -1.15550087, -1.27755803, -1.27516159, -1.29312858, -1.27371347]) . Test Score for the default SVR . print(svr.score(x_test,y_test)) . -1.0037826636263079 . 5 - Fold Cross Validation . from sklearn.model_selection import KFold x = np.array(x_train + x_test) y = np.array(y_train.tolist() + y_test.tolist()) . The following method of 5-fold cross validation for finding the ideal parameter, is repeated across all the experiments with different AAindexes. . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . def avg(x): return sum(x)/len(x) . scores_train = {} scores_test = {} for i in parameters: n = 0 svr = SVR(kernel = i) demo_test = [] demo_train = [] while n &lt; 5: x_train_k = train_sets_x[n] y_train_k = train_sets_y[n] x_test_k = test_sets_x[n] y_test_k = test_sets_y[n] svr.fit(x_train_k,y_train_k) demo_train.append(svr.score(x_train_k,y_train_k)) demo_test.append(svr.score(x_test_k,y_test_k)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . Average Training Scores for each of the Kernels . scores_train . {&#39;linear&#39;: 0.9094324956856991, &#39;poly&#39;: 0.9666186481450667, &#39;rbf&#39;: 0.8924190599961588, &#39;sigmoid&#39;: 0.03987006846426018} . Average Test Scores for each of the Kernels . scores_test . {&#39;linear&#39;: 0.828073023893024, &#39;poly&#39;: 0.8944167592225953, &#39;rbf&#39;: 0.8514916868496585, &#39;sigmoid&#39;: -0.4109411142456857} . From the above scores, it is evident that the polynomial kernel performs the best. So, we are going to go with that. . Training Score for the polynomial kernel . svr = SVR(kernel = &quot;poly&quot;) svr.fit(x_train,y_train) print(svr.score(x_train,y_train)) . 0.8755959967194292 . Test Score for the polynomial kernel . print(svr.score(x_test,y_test)) . 0.6723543903416969 . svr.predict(x_test) . array([-1.46114467, -1.02375865, -1.40385193, -1.67735176, -1.50871657, -1.93812487, -1.51127952, -1.04666127, -1.57058112, -1.01227782, -1.44309145, -0.97118722, -1.43612546, -1.72874969, -2.25041533, -1.72276683, -2.00312591, -1.55897144, -2.02189508, -1.61201774, -1.03372668, -1.59476383, -1.47486817, -1.77391459, -2.3208858 , -2.33256065, -2.08760292, -2.40336772]) . The epistatic interaction study of all the different amino acid indexes will be presented in a later post. . Trying Fourier Transform: . The main objective for performing Fourier Transform is to introduce a method that can help capture epistasis. The crux of Fourier Transform is its ability to turn the numerical sequence into a spectrum. The spectrum created by the method will vary entirely if even one amino acid is replace by another, thus capturing the impact of the mutation on a global level. . We will be using the fft package in Scientific Python(Scipy). . from scipy.fft import fft . The Fourier transformed sequence will be a complex number, with a real and imaginary half. . In our case, we will only be using the real values of the spectrum created by the Fourier Transform. . fft(x_train[0]).real . array([ 8.15 , 1.73785602, -1.4582119 , 2.285 , 0.24535588, 0.24535588, 2.285 , -1.4582119 , 1.73785602]) . fft(x_test[0]).real . array([ 7.54 , 1.71102508, -0.54688746, 3.325 , 0.83086238, 0.83086238, 3.325 , -0.54688746, 1.71102508]) . fft_x_train = fft(x_train).real.tolist() fft_x_test = fft(x_test).real.tolist() . Lets do the model . from sklearn.linear_model import LinearRegression reg = LinearRegression() . reg.fit(fft_x_train,y_train) . LinearRegression() . Training Score for Linear Regression with FFT . reg.score(fft_x_train,y_train) . 0.768712883858722 . prediction_fft = reg.predict(fft_x_test) . prediction_fft . array([-1.70032707, -1.03948807, -1.60929966, -1.86214102, -1.60601353, -2.43195262, -1.77180701, -0.94517453, -1.77111361, -1.110968 , -1.51498612, -0.85484051, -1.6807796 , -1.76782749, -2.59376657, -1.93362096, -2.33763908, -1.67749347, -2.50343256, -1.67680008, -1.01665447, -1.84259355, -1.58646606, -1.83930742, -2.49945303, -2.66524651, -2.40911902, -2.57093297]) . Test Score for Linear Regression with FFT . from sklearn.metrics import r2_score r2_score(y_test,prediction_fft) . 0.7832096880762189 . Even if its a simple linear regression , we are getting much better results by Fourier Transforming the numerical sequences. . Iterative Regression . The interative process being followed here is an exact replica of the one shown in the iterative regression section of the blog. . from sklearn.model_selection import train_test_split from scipy.stats import pearsonr import matplotlib.pyplot as plt . x = x_train + x_test y = y_train.tolist() + y_test.tolist() . list_train_error = [] list_test_error = [] list_test_pr = [] for i in range(0,100): #The random state is varying x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) reg = LinearRegression() reg.fit(x_train,y_train) prediction = reg.predict(x_test) list_train_error.append(reg.score(x_train,y_train)) list_test_error.append(reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction) list_test_pr.append(corr) . plt.plot(range(0,100),list_train_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . The results dont look very promising. Lets try using the one-hot encoded data and see what we get: . Iterative Regression for Fourier Transform . x_fft = fft(x).real #Taking only the real part of the fourier transform . list_train_error_fft = [] list_test_error_fft = [] list_test_pr_fft = [] for i in range(0,100): #The random state is varying x_train,x_test,y_train,y_test = train_test_split(x_fft,y,test_size = 0.25,random_state = i) reg = LinearRegression() reg.fit(x_train,y_train) prediction = reg.predict(x_test) list_train_error_fft.append(reg.score(x_train,y_train)) list_test_error_fft.append(reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction) list_test_pr_fft.append(corr) . plt.plot(range(0,100),list_train_error_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . We can very clearly see some very impactful difference if we use Fourier Transform, with the model being more stable across iterations - and this is for a simple linear regression. . SVM with Hydrophobicity Index and Fourier Transform . Parameters considered for 5-Fold Cross Validation . parameters = [&#39;linear&#39;,&#39;poly&#39;,&#39;rbf&#39;,&#39;sigmoid&#39;] . from sklearn.model_selection import KFold from sklearn.svm import SVR from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score . y_fft = np.array(y) . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x_fft): train_sets_x.append(x_fft[train]) train_sets_y.append(y_fft[train]) test_sets_x.append(x_fft[test]) test_sets_y.append(y_fft[test]) . def avg(x): return sum(x)/len(x) . scores_train = {} scores_test = {} for i in parameters: n = 0 svr = SVR(kernel = i) demo_test = [] demo_train = [] while n &lt; 5: x_train_k = train_sets_x[n] y_train_k = train_sets_y[n] x_test_k = test_sets_x[n] y_test_k = test_sets_y[n] svr.fit(x_train_k,y_train_k) demo_train.append(svr.score(x_train_k,y_train_k)) demo_test.append(svr.score(x_test_k,y_test_k)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . Average Training Scores for each of the Kernels . print(scores_train) . {&#39;linear&#39;: 0.8846354784197722, &#39;poly&#39;: 0.8514339879569572, &#39;rbf&#39;: 0.7871443479542013, &#39;sigmoid&#39;: 0.4729345728207631} . Average Test Scores for each of the Kernels . print(scores_test) . {&#39;linear&#39;: 0.8303784300502087, &#39;poly&#39;: 0.7830411197739757, &#39;rbf&#39;: 0.7242712633776763, &#39;sigmoid&#39;: 0.31122856700085044} . Linear seems to be the best . Iterative SVR Regression . list_train_error_svr_fft = [] list_test_error_svr_fft = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) svr = SVR(kernel = &#39;linear&#39;) svr.fit(x_train,y_train) list_train_error_svr_fft.append(svr.score(x_train,y_train)) list_test_error_svr_fft.append(svr.score(x_test,y_test)) . plt.plot(range(0,100),list_train_error_svr_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_svr_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . SVR gives us much more promising results when in tandem with FFT. . Now, lets test it on the original train and test dataset split . Training Score for Linear SVR on the multi-mutant test set . svr_fft = SVR(kernel = &quot;linear&quot;) svr_fft.fit(fft_x_train,train_data[&quot;G&quot;]) svr_fft.score(fft_x_train,train_data[&quot;G&quot;]) . 0.7160190789755422 . svr_fft.predict(fft_x_test) . array([-1.53731892, -0.98034462, -1.62772786, -1.58501003, -1.4747318 , -2.23239327, -1.5589967 , -0.91775749, -1.67541896, -1.0020224 , -1.56514073, -0.89174416, -1.64940563, -1.5224229 , -2.28008437, -1.60668781, -2.16980614, -1.49640957, -2.25407104, -1.61283184, -0.93943527, -1.69709674, -1.5868185 , -1.54410068, -2.21749725, -2.30176215, -2.19148391, -2.23917502]) . epi_result = pd.read_csv(&quot;svr_hydrophobicity_fourier.csv&quot;,encoding=&#39;cp1252&#39;) epi_result . Additive value Ground Truth Prediction Status . 0 -1.82 | -1.680 | -1.537319 | Predicts Negative | . 1 -1.08 | -0.870 | -0.980345 | Predicts Negative | . 2 -1.50 | -1.680 | -1.627728 | Predicts Positive | . 3 -2.05 | -1.840 | -1.585010 | Predicts Negative | . 4 -1.94 | -1.670 | -1.474732 | Predicts Negative | . 5 -2.47 | -2.190 | -2.232393 | Predicts Negative | . 6 -1.82 | -1.930 | -1.558997 | Doesn’t Predict Positive | . 7 -1.20 | -0.900 | -0.917757 | Predicts Negative | . 8 -1.73 | -1.300 | -1.675419 | Predicts Negative | . 9 -1.08 | -0.980 | -1.002022 | Predicts Negative | . 10 -1.62 | -1.730 | -1.565141 | Doesn’t Predict Positive | . 11 -0.97 | -0.890 | -0.891744 | Predicts Negative | . 12 -1.50 | -1.880 | -1.649406 | Predicts Positive | . 13 -2.17 | -1.920 | -1.522423 | Predicts Negative | . 14 -2.70 | -2.150 | -2.280084 | Predicts Negative | . 15 -2.05 | -1.960 | -1.606688 | Predicts Negative | . 16 -2.59 | -2.410 | -2.169806 | Predicts Negative | . 17 -1.94 | -1.850 | -1.496410 | Predicts Negative | . 18 -2.47 | -2.370 | -2.254071 | Predicts Negative | . 19 -1.85 | -1.510 | -1.612832 | Predicts Negative | . 20 -1.20 | -0.920 | -0.939435 | Predicts Negative | . 21 -1.73 | -1.750 | -1.697097 | Doesn’t Predict Positive | . 22 -1.62 | -1.740 | -1.586818 | Doesn’t Predict Positive | . 23 -2.17 | -2.570 | -1.544101 | Doesn’t Predict Positive | . 24 -2.82 | -2.090 | -2.217497 | Predicts Negative | . 25 -2.70 | -2.320 | -2.301762 | Predicts Negative | . 26 -2.59 | -2.773 | -2.191484 | Doesn’t Predict Positive | . 27 -2.82 | -2.870 | -2.239175 | Doesn’t Predict Positive | . We can see that this model predicts all the negative epistasis instances and a few postive epistasis instances - an improvement from last time, proving the strength of Fourier Transform . Test Score for Linear SVR on the multi-mutant test set . svr_fft.score(fft_x_test,test_data[&quot;G&quot;]) . 0.6651930940890364 . Conclusion: . Just by changing the AAindex and incorporating Fourier Transform, we obtained some promising results. As we stated before, R2 score is not the main criteria for deciding on a superior model; its going to be a combination of R2 score and the capability to predict the correct epistatic state. . We repeated this experiment on a selected few AAindexes which we felt may be pertinent to this problem. In my next post, we will go through the results obtained and choose the best performing model. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-4%20-%20hydrophobicity%20index%20and%20fft/2022/07/27/Hydrophobicity_index_and_Fourier_Transform.html",
            "relUrl": "/part-4%20-%20hydrophobicity%20index%20and%20fft/2022/07/27/Hydrophobicity_index_and_Fourier_Transform.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Alternate AA index and Fourier Transform",
            "content": "Alternate AAindex - Hydrophobicity . In this post, we will be trying an alternative AAindex - Amino Acid Index - called Hydrophobicity index and also look into incorporating Fourier Transform into the proceedings. . The First Section will just be a repeat of the Data Engineering process already introduced in the previous sections of the blog, with the only edit being that we are using a different AAindex. So, please feel free to skip to the Linear Regression section. . import pandas as pd import numpy as np . h_index = pd.read_csv(&quot;hydrophobicity_index.csv&quot;) . h_index.iloc[:1] . A R N D C Q E G H I L K M F P S T W Y V . 0 0.61 | 0.6 | 0.06 | 0.46 | 1.07 | 0.0 | 0.47 | 0.07 | 0.61 | 2.22 | 1.53 | 1.15 | 1.18 | 2.02 | 1.95 | 0.05 | 0.05 | 2.65 | 1.88 | 1.32 | . h_dict = {} for i in h_index.columns: h_dict[i] = h_index[i][0] . Dictionary with the Hydrophobicity indexes . h_dict . {&#39;A&#39;: 0.61, &#39;R&#39;: 0.6, &#39;N&#39;: 0.06, &#39;D&#39;: 0.46, &#39;C&#39;: 1.07, &#39;Q&#39;: 0.0, &#39;E&#39;: 0.47, &#39;G&#39;: 0.07, &#39;H&#39;: 0.61, &#39;I&#39;: 2.22, &#39;L&#39;: 1.53, &#39;K&#39;: 1.15, &#39;M&#39;: 1.18, &#39;F&#39;: 2.02, &#39;P&#39;: 1.95, &#39;S&#39;: 0.05, &#39;T&#39;: 0.05, &#39;W&#39;: 2.65, &#39;Y&#39;: 1.88, &#39;V&#39;: 1.32} . train_data = pd.read_csv(&quot;ee_train.csv&quot;) test_data = pd.read_csv(&quot;ee_test.csv&quot;) print(train_data) print(test_data) . Type Sequence EE G 0 WT LARLTTMLC 4 -0.85 1 1 FARLTTMLC 12 -1.50 2 2 LNRLTTMLC 7 -1.17 3 3 LASLTTMLC 4 -0.85 4 4 LARYTTMLC 4 -0.85 5 5 LARLWTMLC 12 -1.50 6 6 LARLTVMLC 4 -0.85 7 7 LARLTTPLC 6 -1.08 8 8 LARLTTMYC 4 -0.85 9 9 LARLTTMLV 5 -0.97 Sequence G 0 FNSLTTMLC -1.68 1 LARLTTPYC -0.87 2 LARLWVMLC -1.68 3 FNSLTTPYC -1.84 4 FNSLTTMLV -1.67 5 FNSLWVMLC -2.19 6 FNSYTTMLC -1.93 7 LARLTTPYV -0.90 8 LARLWVPYC -1.30 9 LARYTTPYC -0.98 10 LARLWVMLV -1.73 11 LARYTTMLV -0.89 12 LARYWVMLC -1.88 13 FNSLTTPYV -1.92 14 FNSLWVPYC -2.15 15 FNSYTTPYC -1.96 16 FNSLWVMLV -2.41 17 FNSYTTMLV -1.85 18 FNSYWVMLC -2.37 19 LARLWVPYV -1.51 20 LARYTTPYV -0.92 21 LARYWVPYC -1.75 22 LARYWVMLV -1.74 23 FNSYTTPYV -2.57 24 FNSLWVPYV -2.09 25 FNSYWVPYC -2.32 26 FNSYWVMLV -2.73 27 FNSYWVPYV -2.87 . def convert_seq(x): list_seq = [] for i in x: list_seq.append(h_dict[i]) return list_seq . print(convert_seq(&#39;LARLTTMLC&#39;)) . [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07] . x_train = list(map(convert_seq,train_data[&quot;Sequence&quot;])) x_test = list(map(convert_seq,test_data[&quot;Sequence&quot;])) . x_train . [[1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [2.02, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.06, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.05, 1.53, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.88, 0.05, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 2.65, 0.05, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 1.32, 1.18, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.95, 1.53, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.88, 1.07], [1.53, 0.61, 0.6, 1.53, 0.05, 0.05, 1.18, 1.53, 1.32]] . y_train = train_data[&quot;G&quot;] . y_test = test_data[&quot;G&quot;] . Simple Linear Regression . from sklearn.linear_model import LinearRegression reg = LinearRegression() . reg.fit(x_train,y_train) . LinearRegression() . prediction = reg.predict(x_test) . print(prediction) . [-1.82 -1.08 -1.5 -2.05 -1.94 -2.47 -1.82 -1.2 -1.73 -1.08 -1.62 -0.97 -1.5 -2.17 -2.7 -2.05 -2.59 -1.94 -2.47 -1.85 -1.2 -1.73 -1.62 -2.17 -2.82 -2.7 -2.59 -2.82] . R2 Score for Test Set . from sklearn.metrics import r2_score r2_score(y_test,prediction) . 0.730824152182818 . As promised, i did try the same simple linear regression model with another AAindex and we get results similar to the one we obtained with the EIIP AAindex and the One-Hot model. This trend continues throughout for the closed form linear regression model, and hence we will not be considering it for our task. . SVR . from sklearn.svm import SVR . The parameters we are considering for SVR . parameters = [&#39;linear&#39;,&#39;poly&#39;,&#39;rbf&#39;,&#39;sigmoid&#39;] . Default SVR . svr = SVR() svr.fit(x_train,y_train) print(svr.score(x_train,y_train)) . 0.6209572424941144 . svr.predict(x_test) . array([-1.17149025, -0.98769153, -1.30984363, -1.16435546, -1.17187482, -1.29976936, -1.15932717, -0.99132529, -1.28701036, -0.98135599, -1.3077824 , -0.95447213, -1.30433144, -1.16486616, -1.27907534, -1.15482163, -1.29788623, -1.15992676, -1.2949261 , -1.28535278, -0.98510175, -1.28256079, -1.30236765, -1.15550087, -1.27755803, -1.27516159, -1.29312858, -1.27371347]) . Test Score for the default SVR . print(svr.score(x_test,y_test)) . -1.0037826636263079 . 5 - Fold Cross Validation . from sklearn.model_selection import KFold x = np.array(x_train + x_test) y = np.array(y_train.tolist() + y_test.tolist()) . The following method of 5-fold cross validation for finding the ideal parameter, is repeated across all the experiments with different AAindexes. . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . def avg(x): return sum(x)/len(x) . scores_train = {} scores_test = {} for i in parameters: n = 0 svr = SVR(kernel = i) demo_test = [] demo_train = [] while n &lt; 5: x_train_k = train_sets_x[n] y_train_k = train_sets_y[n] x_test_k = test_sets_x[n] y_test_k = test_sets_y[n] svr.fit(x_train_k,y_train_k) demo_train.append(svr.score(x_train_k,y_train_k)) demo_test.append(svr.score(x_test_k,y_test_k)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . Average Training Scores for each of the Kernels . scores_train . {&#39;linear&#39;: 0.9094324956856991, &#39;poly&#39;: 0.9666186481450667, &#39;rbf&#39;: 0.8924190599961588, &#39;sigmoid&#39;: 0.03987006846426018} . Average Test Scores for each of the Kernels . scores_test . {&#39;linear&#39;: 0.828073023893024, &#39;poly&#39;: 0.8944167592225953, &#39;rbf&#39;: 0.8514916868496585, &#39;sigmoid&#39;: -0.4109411142456857} . From the above scores, it is evident that the polynomial kernel performs the best. So, we are going to go with that. . Training Score for the polynomial kernel . svr = SVR(kernel = &quot;poly&quot;) svr.fit(x_train,y_train) print(svr.score(x_train,y_train)) . 0.8755959967194292 . Test Score for the polynomial kernel . print(svr.score(x_test,y_test)) . 0.6723543903416969 . svr.predict(x_test) . array([-1.46114467, -1.02375865, -1.40385193, -1.67735176, -1.50871657, -1.93812487, -1.51127952, -1.04666127, -1.57058112, -1.01227782, -1.44309145, -0.97118722, -1.43612546, -1.72874969, -2.25041533, -1.72276683, -2.00312591, -1.55897144, -2.02189508, -1.61201774, -1.03372668, -1.59476383, -1.47486817, -1.77391459, -2.3208858 , -2.33256065, -2.08760292, -2.40336772]) . The epistatic interaction study of all the different amino acid indexes will be presented in a later post. . Trying Fourier Transform: . The main objective for performing Fourier Transform is to introduce a method that can help capture epistasis. The crux of Fourier Transform is its ability to turn the numerical sequence into a spectrum. The spectrum created by the method will vary entirely if even one amino acid is replace by another, thus capturing the impact of the mutation on a global level. . We will be using the fft package in Scientific Python(Scipy). . from scipy.fft import fft . The Fourier transformed sequence will be a complex number, with a real and imaginary half. . In our case, we will only be using the real values of the spectrum created by the Fourier Transform. . fft(x_train[0]).real . array([ 8.15 , 1.73785602, -1.4582119 , 2.285 , 0.24535588, 0.24535588, 2.285 , -1.4582119 , 1.73785602]) . fft(x_test[0]).real . array([ 7.54 , 1.71102508, -0.54688746, 3.325 , 0.83086238, 0.83086238, 3.325 , -0.54688746, 1.71102508]) . fft_x_train = fft(x_train).real.tolist() fft_x_test = fft(x_test).real.tolist() . Lets do the model . from sklearn.linear_model import LinearRegression reg = LinearRegression() . reg.fit(fft_x_train,y_train) . LinearRegression() . Training Score for Linear Regression with FFT . reg.score(fft_x_train,y_train) . 0.768712883858722 . prediction_fft = reg.predict(fft_x_test) . prediction_fft . array([-1.70032707, -1.03948807, -1.60929966, -1.86214102, -1.60601353, -2.43195262, -1.77180701, -0.94517453, -1.77111361, -1.110968 , -1.51498612, -0.85484051, -1.6807796 , -1.76782749, -2.59376657, -1.93362096, -2.33763908, -1.67749347, -2.50343256, -1.67680008, -1.01665447, -1.84259355, -1.58646606, -1.83930742, -2.49945303, -2.66524651, -2.40911902, -2.57093297]) . Test Score for Linear Regression with FFT . from sklearn.metrics import r2_score r2_score(y_test,prediction_fft) . 0.7832096880762189 . Even if its a simple linear regression , we are getting much better results by Fourier Transforming the numerical sequences. . Iterative Regression for Simple Linear Regression . The interative process being followed here is an exact replica of the one shown in the interative regression section of the blog. . from sklearn.model_selection import train_test_split from scipy.stats import pearsonr import matplotlib.pyplot as plt . x = x_train + x_test y = y_train.tolist() + y_test.tolist() . list_train_error = [] list_test_error = [] list_test_pr = [] for i in range(0,100): #The random state is varying x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) reg = LinearRegression() reg.fit(x_train,y_train) prediction = reg.predict(x_test) list_train_error.append(reg.score(x_train,y_train)) list_test_error.append(reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction) list_test_pr.append(corr) . plt.plot(range(0,100),list_train_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . The results dont look very promising. Lets try using the one-hot encoded data and see what we get: . Iterative Regression for Fourier Transform . x_fft = fft(x).real . list_train_error_fft = [] list_test_error_fft = [] list_test_pr_fft = [] for i in range(0,100): #The random state is varying x_train,x_test,y_train,y_test = train_test_split(x_fft,y,test_size = 0.25,random_state = i) reg = LinearRegression() reg.fit(x_train,y_train) prediction = reg.predict(x_test) list_train_error_fft.append(reg.score(x_train,y_train)) list_test_error_fft.append(reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction) list_test_pr_fft.append(corr) . plt.plot(range(0,100),list_train_error_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . We can very clearly see some very impactful difference if we use Fourier Transform, with the model being more stable across iterations . SVM with Hydrophobicity Fourier . Parameters considered for 5-Fold Cross Validation . parameters = [&#39;linear&#39;,&#39;poly&#39;,&#39;rbf&#39;,&#39;sigmoid&#39;] . from sklearn.model_selection import KFold from sklearn.svm import SVR from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score . y_fft = np.array(y) . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x_fft): train_sets_x.append(x_fft[train]) train_sets_y.append(y_fft[train]) test_sets_x.append(x_fft[test]) test_sets_y.append(y_fft[test]) . def avg(x): return sum(x)/len(x) . scores_train = {} scores_test = {} for i in parameters: n = 0 svr = SVR(kernel = i) demo_test = [] demo_train = [] while n &lt; 5: x_train_k = train_sets_x[n] y_train_k = train_sets_y[n] x_test_k = test_sets_x[n] y_test_k = test_sets_y[n] svr.fit(x_train_k,y_train_k) demo_train.append(svr.score(x_train_k,y_train_k)) demo_test.append(svr.score(x_test_k,y_test_k)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . Average Training Scores for each of the Kernels . print(scores_train) . {&#39;linear&#39;: 0.8846354784197722, &#39;poly&#39;: 0.8514339879569572, &#39;rbf&#39;: 0.7871443479542013, &#39;sigmoid&#39;: 0.4729345728207631} . Average Test Scores for each of the Kernels . print(scores_test) . {&#39;linear&#39;: 0.8303784300502087, &#39;poly&#39;: 0.7830411197739757, &#39;rbf&#39;: 0.7242712633776763, &#39;sigmoid&#39;: 0.31122856700085044} . Linear seems to be the best . Iterative SVR Regression . list_train_error_svr_fft = [] list_test_error_svr_fft = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) svr = SVR(kernel = &#39;linear&#39;) svr.fit(x_train,y_train) list_train_error_svr_fft.append(svr.score(x_train,y_train)) list_test_error_svr_fft.append(svr.score(x_test,y_test)) . plt.plot(range(0,100),list_train_error_svr_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_svr_fft , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . SVR gives us much more promising results when in tandem with FFT . Now, lets test it on the original train and test dataset split . Training Score for Linear SVR on the multi-mutant test set . svr_fft = SVR(kernel = &quot;linear&quot;) svr_fft.fit(fft_x_train,train_data[&quot;G&quot;]) svr_fft.score(fft_x_train,train_data[&quot;G&quot;]) . 0.7160190789755422 . svr_fft.predict(fft_x_test) . array([-1.53731892, -0.98034462, -1.62772786, -1.58501003, -1.4747318 , -2.23239327, -1.5589967 , -0.91775749, -1.67541896, -1.0020224 , -1.56514073, -0.89174416, -1.64940563, -1.5224229 , -2.28008437, -1.60668781, -2.16980614, -1.49640957, -2.25407104, -1.61283184, -0.93943527, -1.69709674, -1.5868185 , -1.54410068, -2.21749725, -2.30176215, -2.19148391, -2.23917502]) . Test Score for Linear SVR on the multi-mutant test set . svr_fft.score(fft_x_test,test_data[&quot;G&quot;]) . 0.6651930940890364 . Conclusion: . Just by changing the AAindex and incorporating Fourier Transform, we obtained some promising results. As we stated before, R2 score is not the main criteria for deciding on a superior model; its going to be a combination of R2 score and the capability to predict the correct epistatic state. . We repeated this experiment on a selected few AAindexes which we felt may be pertinent to this problem. In my next post, we will go through the results obtained and choose the best performing model. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-4%20-%20hydrophobicity%20index%20and%20fft/2022/07/27/Hydrophobicity-index-and-Fourier-Transform.html",
            "relUrl": "/part-4%20-%20hydrophobicity%20index%20and%20fft/2022/07/27/Hydrophobicity-index-and-Fourier-Transform.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "ML Models for Protein Studies: SVR",
            "content": "In the previous post, we explored the stability and performance of some regularised regression models. As expected, they seemed to showcase a higher stability than the closed form Linear Regression model when tested on unseen datapoints across iterations. Now , let us explore another Regression Model called Support Vector Regression , which is a derivative of the very popular Support Vector Machine model. . Support Vector Machines . The objective of a SVM model is to find the ideal hyperplane that best separates the points found in space. If the datapoints are of N-dimension , the hyperplane is of the shape N-1 dimension. . Why not linear classifiers? . Linear classifiers may not be the perfect model for problems, as they use all the points in the dataset to determine the decision boundary. Also , if you try and visualise, there are multiple decision boundaries that are possible in a linear model. How can we determine the best one? . This is where SVM’s classifiers come in: They choose the best decision boundary out of all the possible ones, and they depend on the “support vectors”. . Support vectors are the points which are the closest to the decision boundary. Since they are the main players in determining the decision boundary, this model handles outliers much better than normal linear models. When this concept is applied in a regression scenario , we get Support Vector Regression. . Code . print(&quot;Total number of x samples: {}&quot;.format(len(x))) print(&quot;Total number of y samples: {}&quot;.format(len(y))) . Total number of x samples: 38 Total number of y samples: 38 . Support Vector Regression with kernels . Kernels help us deal with situations where the data doesn’t show linearly separability. In simple terms, they work by projecting the points in some higher dimensional space where the data is linearly separable. . Choosing a specific kernel is a hyperparameter selection task. So, we will be using Cross-Validation to choose the kernel which works best for us. . parameters = [&#39;linear&#39;,&#39;poly&#39;,&#39;rbf&#39;,&#39;sigmoid&#39;] . from sklearn.model_selection import KFold from sklearn.svm import SVR from sklearn.model_selection import train_test_split from sklearn.metrics import r2_score x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25, random_state = 40) . svr = SVR() svr.fit(x_train,y_train) . SVR() . prediction = svr.predict(x_test) . Training Score for default SVR model . print(svr.score(x_train,y_train)) . 0.9549930990271551 . Test Score for default SVR model . print(r2_score(y_test,prediction)) . 0.6791473960006742 . 5-Fold Cross Validation for Kernel Selection . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . train_sets_x[3].shape . (31, 9) . 0,1,2 -&gt; 30 Training samples . 3,4 -&gt; 31 Training samples . def avg(x): return sum(x)/len(x) . scores_train = {} scores_test = {} for i in parameters: n = 0 svr = SVR(kernel = i) demo_test = [] demo_train = [] while n &lt; 5: x_train = train_sets_x[n] y_train = train_sets_y[n] x_test = test_sets_x[n] y_test = test_sets_y[n] svr.fit(x_train,y_train) demo_train.append(svr.score(x_train,y_train)) demo_test.append(svr.score(x_test,y_test)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . Average Training Scores for each of the Kernels . scores_train . {&#39;linear&#39;: 0.18717649524787233, &#39;poly&#39;: 0.9776295493358889, &#39;rbf&#39;: 0.959033206629871, &#39;sigmoid&#39;: 0.45755598340290715} . Average Test Scores for each of the kernels . scores_test . {&#39;linear&#39;: -0.20770377534740375, &#39;poly&#39;: 0.7565565734579883, &#39;rbf&#39;: 0.7762069885373662, &#39;sigmoid&#39;: 0.2609505071869702} . RBF seems to give a better test result. So, lets go with that . Iterative SVR . list_train_error_svr = [] list_test_error_svr = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) svr = SVR(kernel = &#39;rbf&#39;) svr.fit(x_train,y_train) list_train_error_svr.append(svr.score(x_train,y_train)) list_test_error_svr.append(svr.score(x_test,y_test)) . plt.plot(range(0,100),list_train_error_svr , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_svr , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Trying with the polynomial kernel . list_train_error_svr_poly = [] list_test_error_svr_poly = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) svr = SVR(kernel = &#39;poly&#39;) svr.fit(x_train,y_train) list_train_error_svr_poly.append(svr.score(x_train,y_train)) list_test_error_svr_poly.append(svr.score(x_test,y_test)) . plt.plot(range(0,100),list_train_error_svr_poly , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_svr_poly , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_svr_poly , c=&quot;red&quot;) plt.plot(range(0,100),list_test_error_svr,c = &quot;blue&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;SVR score&quot;) plt.title(&quot;Comparing Test scores of Poly and RBF kernel&quot;) plt.legend([&quot;Poly&quot;,&quot;RBF&quot;]) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(*args, **kw)&gt; . Doesnt look like there is much difference between poly and rbf kernel - fits with what we saw in the Cross-Validation. . Epistatic Analysis . x_train = [num_convert(x) for x in data[&quot;Sequence&quot;]] x_test = [num_convert(x) for x in data_test[&quot;Sequence&quot;]] y_train = data[&quot;G&quot;] y_test = data_test[&quot;G&quot;] . epi_svr = SVR(kernel = &quot;rbf&quot;) epi_svr.fit(x_train,y_train) epi_svr.score(x_train,y_train) . 0.6951760830021503 . prediction_svr_epi = epi_svr.predict(x_test) print(epi_svr.score(x_test,y_test)) . -1.0092743268789817 . prediction_svr_epi . array([-1.42568631, -1.00555014, -1.083857 , -1.32499656, -1.35048049, -1.32667435, -1.36342529, -1.06702523, -1.10228544, -1.00413899, -1.11985689, -1.02955882, -1.07518474, -1.28254766, -1.26974212, -1.28776369, -1.28367962, -1.30847455, -1.29189626, -1.1322901 , -1.06607317, -1.09997235, -1.11400594, -1.25742758, -1.24526888, -1.24888344, -1.26021572, -1.23119606]) . Further analysis of the predictions only led to the conclusion that the model doesn&#39;t do a very good job of predicting epistasis. . Conclusive Thoughts: . From the iterative regression graphs, we can see that the stability of the SVR model is quite respectable. However, the epistatic analysis results arnt equally promising. Having said that, we will be revisiting SVRs when we explore Fourier Transform. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-3%20-%20support_vector_regression/2022/06/29/SVR.html",
            "relUrl": "/part-3%20-%20support_vector_regression/2022/06/29/SVR.html",
            "date": " • Jun 29, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "ML Models for Protein Studies: Regularised Regression",
            "content": "In my previous post, we learnt that a simple closed form regression is not the way to go when it comes to epistatic studies. In this post, i will be trying some regularised regression models, along with testing the consistency of the models across many iterations. The hope is to see which of these models is more consistent across different mixes and matches of our dataset. . import pandas as pd import numpy as np import matplotlib.pyplot as plt . data = pd.read_csv(&quot;ee_train.csv&quot;) data_test = pd.read_csv(&quot;ee_test.csv&quot;) data.head() . We are still going to be continuing with EIIP. We will move on to other physiochemical properties in my future posts. . dict_eiip = {&#39;A&#39;: 0.0373, &#39;R&#39;: 0.0959, &#39;N&#39;: 0.0036, &#39;D&#39;: 0.1263, &#39;C&#39;: 0.0829, &#39;Q&#39;: 0.0761, &#39;E&#39;: 0.0058, &#39;G&#39;: 0.005, &#39;H&#39;: 0.0242, &#39;I&#39;: 0, &#39;L&#39;: 0, &#39;K&#39;: 0.0371, &#39;M&#39;: 0.0823, &#39;F&#39;: 0.0946, &#39;P&#39;: 0.0198, &#39;S&#39;: 0.0829, &#39;T&#39;: 0.0941, &#39;W&#39;: 0.0548, &#39;Y&#39;: 0.0516, &#39;V&#39;: 0.0057} . def num_convert(x): return [dict_eiip[i] for i in x] . x_train = [num_convert(x) for x in data[&quot;Sequence&quot;]] x_test = [num_convert(x) for x in data_test[&quot;Sequence&quot;]] . y_train = data[&quot;G&quot;] y_test = data_test[&quot;G&quot;] . x = np.append(x_train,x_test,axis = 0) y = np.append(y_train,y_test) . print(&quot;Total number of x samples: {}&quot;.format(len(x))) print(&quot;Total number of y samples: {}&quot;.format(len(y))) . Total number of x samples: 38 Total number of y samples: 38 . Linear Regression Model . from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split . Now, what happens if we combine the training and test dataset to do a random train-test split for testing a simple linear regressor? . x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 40) . reg = LinearRegression() reg.fit(x_train,y_train) . LinearRegression() . prediction_0 = reg.predict(x_test) . R2 Score for random Training Set . reg.score(x_train,y_train) #The training score seems to be a little too high . 0.9578526758602008 . R2 score for random Test Set . print(r2_score(y_test,prediction_0)) . 0.7155946136422272 . from scipy.stats import pearsonr corr, _ = pearsonr(y_test, prediction_0) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.868 . print(reg.score(x_test,y_test)) . 0.7155946136422272 . Regression with Iterations . Creating 100 different iterations of training and test sets from the 38 entry dataset . Training and testing the Regression Model on each of them , while recording training and test scores . list_train_error = [] list_test_error = [] list_test_pr = [] for i in range(0,100): #The random state is varying x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) reg = LinearRegression() reg.fit(x_train,y_train) prediction = reg.predict(x_test) list_train_error.append(reg.score(x_train,y_train)) list_test_error.append(reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction) list_test_pr.append(corr) . print(len(list_train_error)) print(len(list_test_error)) print(len(list_test_pr)) . 100 100 100 . list_test_error[41] . -2.9262063917582427e+26 . plt.plot(range(0,100),list_train_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson Score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Ridge Regression . Ridge Regression is a regularized regression model. To put in simply: The normal method of minimizing error will lead to overfitting and the weights of the regression model will become large. Hence, we will add the term &quot;|w|^2*alpha/2&quot; - where w is the weight vector and alpha is the hyper parameter- to minimize the weights while we minimize error. . parameters = {&#39;alpha&#39;:[0.01,0.02,0.03,0.04,0.05,0.1,0.5,1,1.5,10]} . 5 fold is the default. Scoring should be done using r2 scores . from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.linear_model import Ridge from sklearn.metrics import r2_score ridge_reg = Ridge() model = GridSearchCV(ridge_reg,param_grid = parameters, scoring = &#39;r2&#39;,verbose = 3) . model.fit(x,y) . Fitting 5 folds for each of 10 candidates, totalling 50 fits [CV 1/5] END .......................alpha=0.01;, score=-0.151 total time= 0.0s [CV 2/5] END ........................alpha=0.01;, score=0.923 total time= 0.0s [CV 3/5] END ........................alpha=0.01;, score=0.758 total time= 0.0s [CV 4/5] END ........................alpha=0.01;, score=0.888 total time= 0.0s [CV 5/5] END .......................alpha=0.01;, score=-0.020 total time= 0.0s [CV 1/5] END .......................alpha=0.02;, score=-0.291 total time= 0.0s [CV 2/5] END ........................alpha=0.02;, score=0.896 total time= 0.0s [CV 3/5] END ........................alpha=0.02;, score=0.700 total time= 0.0s [CV 4/5] END ........................alpha=0.02;, score=0.861 total time= 0.0s [CV 5/5] END .......................alpha=0.02;, score=-0.299 total time= 0.0s [CV 1/5] END .......................alpha=0.03;, score=-0.519 total time= 0.0s [CV 2/5] END ........................alpha=0.03;, score=0.861 total time= 0.0s [CV 3/5] END ........................alpha=0.03;, score=0.648 total time= 0.0s [CV 4/5] END ........................alpha=0.03;, score=0.818 total time= 0.0s [CV 5/5] END .......................alpha=0.03;, score=-0.549 total time= 0.0s [CV 1/5] END .......................alpha=0.04;, score=-0.773 total time= 0.0s [CV 2/5] END ........................alpha=0.04;, score=0.823 total time= 0.0s [CV 3/5] END ........................alpha=0.04;, score=0.601 total time= 0.0s [CV 4/5] END ........................alpha=0.04;, score=0.770 total time= 0.0s [CV 5/5] END .......................alpha=0.04;, score=-0.773 total time= 0.0s [CV 1/5] END .......................alpha=0.05;, score=-1.028 total time= 0.0s [CV 2/5] END ........................alpha=0.05;, score=0.785 total time= 0.0s [CV 3/5] END ........................alpha=0.05;, score=0.558 total time= 0.0s [CV 4/5] END ........................alpha=0.05;, score=0.721 total time= 0.0s [CV 5/5] END .......................alpha=0.05;, score=-0.972 total time= 0.0s [CV 1/5] END ........................alpha=0.1;, score=-2.110 total time= 0.0s [CV 2/5] END .........................alpha=0.1;, score=0.615 total time= 0.0s [CV 3/5] END .........................alpha=0.1;, score=0.395 total time= 0.0s [CV 4/5] END .........................alpha=0.1;, score=0.503 total time= 0.0s [CV 5/5] END ........................alpha=0.1;, score=-1.700 total time= 0.0s [CV 1/5] END ........................alpha=0.5;, score=-4.792 total time= 0.0s [CV 2/5] END .........................alpha=0.5;, score=0.144 total time= 0.0s [CV 3/5] END ........................alpha=0.5;, score=-0.000 total time= 0.0s [CV 4/5] END ........................alpha=0.5;, score=-0.102 total time= 0.0s [CV 5/5] END ........................alpha=0.5;, score=-3.221 total time= 0.0s [CV 1/5] END ..........................alpha=1;, score=-5.454 total time= 0.0s [CV 2/5] END ...........................alpha=1;, score=0.017 total time= 0.0s [CV 3/5] END ..........................alpha=1;, score=-0.103 total time= 0.0s [CV 4/5] END ..........................alpha=1;, score=-0.264 total time= 0.0s [CV 5/5] END ..........................alpha=1;, score=-3.568 total time= 0.0s [CV 1/5] END ........................alpha=1.5;, score=-5.705 total time= 0.0s [CV 2/5] END ........................alpha=1.5;, score=-0.032 total time= 0.0s [CV 3/5] END ........................alpha=1.5;, score=-0.142 total time= 0.0s [CV 4/5] END ........................alpha=1.5;, score=-0.327 total time= 0.0s [CV 5/5] END ........................alpha=1.5;, score=-3.697 total time= 0.0s [CV 1/5] END .........................alpha=10;, score=-6.173 total time= 0.0s [CV 2/5] END .........................alpha=10;, score=-0.126 total time= 0.0s [CV 3/5] END .........................alpha=10;, score=-0.217 total time= 0.0s [CV 4/5] END .........................alpha=10;, score=-0.445 total time= 0.0s [CV 5/5] END .........................alpha=10;, score=-3.936 total time= 0.0s . GridSearchCV(estimator=Ridge(), param_grid={&#39;alpha&#39;: [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.5, 1, 1.5, 10]}, scoring=&#39;r2&#39;, verbose=3) . The best parameters . print(model.best_params_) print(model.best_score_) . {&#39;alpha&#39;: 0.01} 0.4797289871671168 . Testing the model . x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 40) . ridge_reg = Ridge(alpha = 0.01) . ridge_reg.fit(x_train,y_train) . Ridge(alpha=0.01) . Training Score . print(ridge_reg.score(x_train,y_train)) . 0.919173242817984 . Test Score . prediction = ridge_reg.predict(x_test) print(r2_score(y_test,prediction)) . 0.6304571598967417 . ridge_reg.score(x_test,y_test) . 0.6304571598967417 . Iterative Ridge Regression . list_train_error_ridge = [] list_test_error_ridge = [] list_test_pr_ridge = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) ridge_reg = Ridge(alpha = 0.01) ridge_reg.fit(x_train,y_train) prediction_ridge = ridge_reg.predict(x_test) list_train_error_ridge.append(ridge_reg.score(x_train,y_train)) list_test_error_ridge.append(ridge_reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction_ridge) list_test_pr_ridge.append(corr) . plt.plot(range(0,100),list_train_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Trying Cross-Validation with my own code - Optional . I tried performing cross validation with my own code. This section can be skipped . parameters = [0.01,0.02,0.03,0.04,0.05,0.1,0.5,1,1.5,10,0] . The plan is simple:&gt;&gt;Create 5 folds of the dataset . Evaluate the performance of all the models on the test set of each fold. Average out the results . Pick out the best parameter . from sklearn.model_selection import KFold . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) . train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . train_sets_x[4].shape # 3 and 4 have 31 samples . (31, 9) . 0,1,2 -&gt; 30 Training samples . 3,4 -&gt; 31 Training samples . def avg(x): return sum(x)/len(x) . We are going to be running the same parameter across the 5 different folds of the data, and collect the scores. We will then average out the scores and store them in a separate dictionary with their respective parameters. . scores_train = {} scores_test = {} for i in parameters: n = 0 ridge_reg = Ridge(alpha = i) demo_test = [] demo_train = [] while n &lt; 5: x_train = train_sets_x[n] y_train = train_sets_y[n] x_test = test_sets_x[n] y_test = test_sets_y[n] ridge_reg.fit(x_train,y_train) demo_train.append(ridge_reg.score(x_train,y_train)) demo_test.append(ridge_reg.score(x_test,y_test)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . Ridge training scores . scores_train . {0.01: 0.880756705051388, 0.02: 0.848759592227608, 0.03: 0.8155867798977818, 0.04: 0.7820866944371316, 0.05: 0.7493107502415729, 0.1: 0.6090778779844153, 0.5: 0.22915121741853933, 1: 0.12776726541632208, 1.5: 0.08852858687863825, 10: 0.014220384963096988, 0: 0.9270556284928301} . Ridge test scores . scores_test . {0.01: 0.7658108034364817, 0.02: 0.7138505286696081, 0.03: 0.6623057644719561, 0.04: 0.6119617120212271, 0.05: 0.5638918169833707, 0.1: 0.36629142156230543, 0.5: -0.13891567744379205, 1: -0.26985323330667554, 1.5: -0.3202372579596836, 10: -0.41525624819926377, 0: 0.8017681404264831} . From what i have obtained above - through 5 fold cross validation - we can see that 0 has the highest test and train scores;this concurs with what we obtained from gridcv. However, since 0 is just normal regression, let us go with the next best:0.01 . list_train_error_ridge = [] list_test_error_ridge = [] list_test_pr_ridge = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) ridge_reg = Ridge(alpha = 0.01) ridge_reg.fit(x_train,y_train) prediction_ridge = ridge_reg.predict(x_test) list_train_error_ridge.append(ridge_reg.score(x_train,y_train)) list_test_error_ridge.append(ridge_reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction_ridge) list_test_pr_ridge.append(corr) . plt.plot(range(0,100),list_train_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Its quite evident that the variance is less with regularized regression . Lasso Regression . from sklearn.linear_model import Lasso . parameters = [0.01,0.02,0.03,0.04,0.05,0.1,0.5,1,1.5,10] . from sklearn.model_selection import KFold kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . scores_train_lasso = {} scores_test_lasso = {} for i in parameters: n = 0 lasso_reg = Lasso(alpha = i) demo_test = [] demo_train = [] while n &lt; 5: x_train = train_sets_x[n] y_train = train_sets_y[n] x_test = test_sets_x[n] y_test = test_sets_y[n] lasso_reg.fit(x_train,y_train) demo_train.append(lasso_reg.score(x_train,y_train)) demo_test.append(lasso_reg.score(x_test,y_test)) n+=1 scores_train_lasso[i] = avg(demo_train) scores_test_lasso[i] = avg(demo_test) . Lasso training scores . scores_train_lasso . {0.01: 0.5512278951745879, 0.02: 0.08042368490101719, 0.03: 0.0, 0.04: 0.0, 0.05: 0.0, 0.1: 0.0, 0.5: 0.0, 1: 0.0, 1.5: 0.0, 10: 0.0} . Lasso test scores . scores_test_lasso . {0.01: 0.3203587893250848, 0.02: -0.32792906301065744, 0.03: -0.43338493165454783, 0.04: -0.43338493165454783, 0.05: -0.43338493165454783, 0.1: -0.43338493165454783, 0.5: -0.43338493165454783, 1: -0.43338493165454783, 1.5: -0.43338493165454783, 10: -0.43338493165454783} . 0.01 seems to give the best result . Iterative Lasso Regression . list_train_error_lasso = [] list_test_error_lasso = [] list_test_pr_lasso = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) lasso_reg = Lasso(alpha = 0.01) lasso_reg.fit(x_train,y_train) prediction_lasso = lasso_reg.predict(x_test) list_train_error_lasso.append(lasso_reg.score(x_train,y_train)) list_test_error_lasso.append(lasso_reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction_lasso) list_test_pr_lasso.append(corr) . plt.plot(range(0,100),list_train_error_lasso , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_lasso , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Testing score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr_lasso , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . So , it is quite clear that ridge regression performs the best when it comes to stability across different iterations of the dataset. So , we will consider ridge regression to be one of the chosen ML models for the task. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-2%20-%20regularised_regression/2022/06/16/Regression_analysis.html",
            "relUrl": "/part-2%20-%20regularised_regression/2022/06/16/Regression_analysis.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Linear Regression Continuation",
            "content": "Multi-Amino Acid mutants in training set . What if we incorporate the multi-mutants in the training set - can we catch the epistatic interactions? . For this, we will created a mixed dataset consisting of 19 point training dataset(9 switched over from the test set) and 19 point test dataset. . from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from scipy.stats import pearsonr . print(len(x_test)) print(len(x_train)) . 28 10 . new_x_train = np.append(x_train,x_test[:9],axis = 0) new_x_test = x_test[9:] . 9 samples are shifted from the test dataset into training, leading us to get 19 samples in each. . print(len(new_x_train)) print(len(new_x_test)) . 19 19 . new_y_train = np.append(y_train,y_test[:9],axis = 0) new_y_test = y_test[9:] . print(len(new_y_train)) print(len(new_y_test)) . 19 19 . reg = LinearRegression() reg.fit(new_x_train,new_y_train) . LinearRegression() . Training Score: . reg.score(new_x_train,new_y_train) . 0.9578724069495184 . Predictions: . prediction_mixed = reg.predict(new_x_test) print(prediction_mixed) . [-0.96354673 -1.44197695 -0.96881818 -1.50982586 -1.72405634 -2.26506402 -1.79190525 -2.27033547 -1.7971767 -2.33818438 -1.42970294 -0.95654417 -1.49755186 -1.5028233 -1.78490269 -2.25806146 -2.32591037 -2.33118182 -2.31890781] . Test Score: . print(r2_score(new_y_test,prediction_mixed)) . 0.733971090323416 . Pearson Correlation Co-efficient: . corr, _ = pearsonr(new_y_test, prediction_mixed) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.909 . plt.scatter(range(1,20),new_y_test,c = &quot;blue&quot;) plt.plot(range(1,20),prediction_mixed , c=&quot;red&quot;) plt.xlabel(&quot;mutant number&quot;) plt.ylabel(&quot;G - value&quot;) plt.legend([&quot;predicted&quot;,&quot;y_test&quot;]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . The r2 scores seem quite identical when compared to the unmixed dataset Linear Regression Model. But, that probably isn&#39;t the best indicator for measuring this model . Now, lets analyse the epistatic interaction prediction of this mixed model: . results = pd.read_csv(&quot;mixed_lr_result.csv&quot;) . results . Actual Additive Predictions Epistatic Prediction . 0 -0.98 | -1.08 | -0.963547 | Predicts negative | . 1 -1.73 | -1.62 | -1.441977 | Doesn’t predict positive | . 2 -0.89 | -0.97 | -0.968818 | Doesn’t predict negative | . 3 -1.88 | -1.50 | -1.509826 | Doesn’t predict positive | . 4 -1.92 | -2.17 | -1.724056 | Predicts negative | . 5 -2.15 | -2.70 | -2.265064 | Predicts negative | . 6 -1.96 | -2.05 | -1.791905 | Predicts negative | . 7 -2.41 | -2.59 | -2.270335 | Predicts negative | . 8 -1.85 | -1.94 | -1.797177 | Predicts negative | . 9 -2.37 | -2.47 | -2.338184 | Predicts negative | . 10 -1.51 | -1.85 | -1.429703 | Predicts negative | . 11 -0.92 | -1.20 | -0.956544 | Predicts negative | . 12 -1.75 | -1.73 | -1.497552 | Doesn’t predict positive | . 13 -1.74 | -1.62 | -1.502823 | Doesn’t predict positive | . 14 -2.57 | -2.17 | -1.784903 | Doesn’t predict positive | . 15 -2.09 | -2.82 | -2.258061 | Predicts negative | . 16 -2.32 | -2.70 | -2.325910 | Predicts negative | . 17 -2.73 | -2.59 | -2.331182 | Doesn’t predict positive | . 18 -2.87 | -2.82 | -2.318908 | Doesn’t predict positive | . We can see that though some negative epistatic interactions are getting predicted, none of the postive interactions are. This leads us to explore some other models which might do a better job when it comes to predicting the correct epistatic state. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-1%20-%20linear_regression/2022/06/15/LR_Continuation.html",
            "relUrl": "/part-1%20-%20linear_regression/2022/06/15/LR_Continuation.html",
            "date": " • Jun 15, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "ML Models for Protein Studies: Introduction",
            "content": "Acknowledgement . Before getting into the actual project, I would like to thank Jahnavi Hunsigida for being a wonderful research partner on the project; and Dr.Sumohana Channappayya of IIT Hyderabad, for giving me an opportunity to work on this project under his mentorship. . Introduction . The beauty of data science lies in the fact that it can be utilised to solve problems in a wide variety of fields. And, if you do have some domain-specific knowledge , you can find that it opens up a wide variety of options that can be taken to solve the problem at hand. . My problem lies in the field of biology, and involves dealing with protein sequences. It is a well-known fact that we have multitudes of protein sequence data available. The basics of biology will tell you that the activity of any protein is dependent not just on its amino acid sequence, but also factors like the way its backbone is designed; the way it folds in 3-D space; the way protein units bind together, etc. So, the challenge lies in being able to manipulate only the sequence data – primary structure , in biological jargon - of proteins in order to build models to solve problems in the field. . The work that follows is a continuation of an already published literature titled , “A machine learning approach for reliable prediction of amino acid interactions and its application in the directed evolution of enantioselective enzymes”, which deals with using only the sequence data to predict a biological parameter called the enantioselectivity value. . Literature Summary: . The paper discusses a methodology for predicting a property – in this case, the enantioselectivity - of the input protein based on just using its primary sequence. This methodology involves using a Digital Signal Processing Technique called innov’SAR , which is a protein structure independent method. First , the protein sequences are converted into spectrums via amino acid index encoding and Fourier Transform. These spectrums are supposed to capture epistatic interactions between the different mutations. The converted inputs are used to design a ML model , which will in turn predict e-values for each of the inputs. The overall goal of this undertaking is to reduce the time spent in studying the activity of newly designed proteins by predicting the activity directly from its sequence. . The authors have already explored and established certain models and parameters for the task at hand. My objective is to try some alternative machine learning models and data manipulations on the same problem and obtain some insights of my own. . The primary steps involved are as follows: . Converting the alphabetical amino-acid sequences into sequences consisting of their respective amino acid indexes. . Using Fourier Transform to obtain information embedded in the converted sequences. . Using these as input in different Machine Learning models , hoping to capture a relationship between the inputs and the ground truth we are trying to predict - in this case , the E-value. . Epistasis: . Before we go into the specifics , I want to talk about an interesting concept called “Epistasis”. Epistasis occurs when the mutation at a certain point in the gene is affected by a mutation at another site. As you will all see, the dataset we are dealing with consists of different mutants ( single and multi-point ) of epoxide hydrolase from Aspergillus Niger (ANEH). Hence , the model we design must be able to account for the epistatic interactions that may occur when we have a sample with mutations at multiple sites. This is especially important when we want to predict the enantioselectivity of unknown mutants. . Data Modelling: . A very vital step of any Machine Learning project involves converting the raw inputs into a form suitable for interpretation by a ML algorithm. This step is especially crucial, as the representation of the input will majorly impact the performance of the model. . Amino Acid Indexes: . The first step involves converting the alphabetical sequences of the inputs into a numerical form which is suitable for use in a machine learning model. For this purpose, we are going to use amino acid indexes. Amino Acid indexes are numerical quantities we assign to each amino acid; it is based on the physical or chemical characteristic we are dealing with ( AAindex: Amino Acid index database ). In this project, I will be trying a variety of AA indexes and comparing their performance in each modelling scenario. . Machine Learning Models: . It should be obvious that the task at hand requires us to predict a continuous value , making this a regression problem. Keeping this is mind , we will focus on the performance of 2 ML regression models: . Linear Regression . | Support Vector Regression . | Dataset: . Set 1: 1 Wild-Type + 9 single point mutants of ANEH . Set 2: 28 Multi-point mutants of ANEH . The E-values for the mutants were measured in the lab. . This dataset was obtained from the supplementary section of the paper. . Basic Linear Regression . We are going to start things off by using basic linear regression to model the problem. The amino acid index we are choosing is called EIIP. . import pandas as pd import numpy as np import matplotlib.pyplot as plt . dict_eiip = {&#39;A&#39;: 0.0373, &#39;R&#39;: 0.0959, &#39;N&#39;: 0.0036, &#39;D&#39;: 0.1263, &#39;C&#39;: 0.0829, &#39;Q&#39;: 0.0761, &#39;E&#39;: 0.0058, &#39;G&#39;: 0.005, &#39;H&#39;: 0.0242, &#39;I&#39;: 0, &#39;L&#39;: 0, &#39;K&#39;: 0.0371, &#39;M&#39;: 0.0823, &#39;F&#39;: 0.0946, &#39;P&#39;: 0.0198, &#39;S&#39;: 0.0829, &#39;T&#39;: 0.0941, &#39;W&#39;: 0.0548, &#39;Y&#39;: 0.0516, &#39;V&#39;: 0.0057} . def num_convert(x): return [dict_eiip[i] for i in x] . data = pd.read_csv(&quot;ee_train.csv&quot;) . data.head() . Type Sequence EE G . 0 WT | LARLTTMLC | 4 | -0.85 | . 1 1 | FARLTTMLC | 12 | -1.50 | . 2 2 | LNRLTTMLC | 7 | -1.17 | . 3 3 | LASLTTMLC | 4 | -0.85 | . 4 4 | LARYTTMLC | 4 | -0.85 | . As we can see, the Sequence column contains the protein sequences found in the training dataset. The ground truth we are dealing with here is a converted value called Free Energy (G). . Formula: . ΔΔG‡=−RT ln (E) . Where E is the enantioselectivity . data.shape . (10, 4) . x_train = [num_convert(x) for x in data[&quot;Sequence&quot;]] . print(x_train) . [[0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0.0946, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0036, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0829, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0.0516, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0548, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0057, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0198, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0.0516, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0057]] . type(x_train[0][0]) . int . y_train = data[&quot;G&quot;] print(y_train) . 0 -0.85 1 -1.50 2 -1.17 3 -0.85 4 -0.85 5 -1.50 6 -0.85 7 -1.08 8 -0.85 9 -0.97 Name: G, dtype: float64 . x_train = np.array(x_train) . print(x_train) . [[0. 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0829] [0.0946 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0036 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0829 0. 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0959 0.0516 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0959 0. 0.0548 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0057 0.0823 0. 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0941 0.0198 0. 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0.0516 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0057]] . The Regression Model . Ordinary Least Square Regression . from sklearn.linear_model import LinearRegression reg = LinearRegression() . reg.fit(x_train,y_train) . LinearRegression() . Predicting the value of the random datapoint . demo_1 = &quot;LARLTTPYC&quot; x_trial = num_convert(demo_1) . x_trial = np.array([x_trial]) print(x_trial) . [[0. 0.0373 0.0959 0. 0.0941 0.0941 0.0198 0.0516 0.0829]] . print(reg.predict(x_trial)) . [-1.08] . Now, lets go to the test dataset . data_test = pd.read_csv(&quot;ee_test.csv&quot;) data_test.head() . Sequence G EE . 0 FNSLTTMLC | -1.68 | 16.29 | . 1 LARLTTPYC | -0.87 | 4.24 | . 2 LARLWVMLC | -1.68 | 16.29 | . 3 FNSLTTPYC | -1.84 | 21.25 | . 4 FNSLTTMLV | -1.67 | 16.02 | . data_test.shape . (28, 3) . x_test = [num_convert(x) for x in data_test[&quot;Sequence&quot;]] . x_test = np.array(x_test) . prediction = reg.predict(x_test) . print(prediction) . [-1.82 -1.08 -1.5 -2.05 -1.94 -2.47 -1.82 -1.2 -1.73 -1.08 -1.62 -0.97 -1.5 -2.17 -2.7 -2.05 -2.59 -1.94 -2.47 -1.85 -1.2 -1.73 -1.62 -2.17 -2.82 -2.7 -2.59 -2.82] . y_test = data_test[&quot;G&quot;] print(y_test) . 0 -1.68 1 -0.87 2 -1.68 3 -1.84 4 -1.67 5 -2.19 6 -1.93 7 -0.90 8 -1.30 9 -0.98 10 -1.73 11 -0.89 12 -1.88 13 -1.92 14 -2.15 15 -1.96 16 -2.41 17 -1.85 18 -2.37 19 -1.51 20 -0.92 21 -1.75 22 -1.74 23 -2.57 24 -2.09 25 -2.32 26 -2.73 27 -2.87 Name: G, dtype: float64 . Test Score . from sklearn.metrics import r2_score r2_score(y_test,prediction) . 0.7308241521828205 . Training Score . reg.score(x_train,y_train) . 1.0 . from scipy.stats import pearsonr corr, _ = pearsonr(y_test, prediction) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.893 . plt.scatter(range(1,29),y_test,c = &quot;blue&quot;) plt.plot(range(1,29),prediction , c=&quot;red&quot;) plt.xlabel(&quot;mutant number&quot;) plt.ylabel(&quot;G - value&quot;) plt.legend([&quot;prediction&quot;,&quot;y_test&quot;]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . From just looking at the R2 score, we can probably say that the model does relatively well for the limited number of datapoints we are dealing with. However , the problem we are dealing with is not quite as simple as getting the line of closest fit due to the problem of epistasis. . To calculate epistasis , we need to see if the e-value obtained due to multiple mutations is lower or higher than the theoretical additive value obtained by adding each individual mutation. Higher leads to positive epistasis; lower leads to negative epistasis. Hence, even if the predicted e-value is close to the actual value , there is the aspect of predicting the correct epistatic state. . Now, when it comes to the linear regression model , the predicted values for the multi-mutant test set is an exact match with the theoretical additive e-values. This observation stays the same regardless of the nature of the amino-acid index we use (This will be proved in later sections of the post, where we try other AA indexes). . Now, to verify if the linear regression model is insensitive to the exact values of the indexes, lets try a one-hot encoding model. . Trying a One-hot Model . def seq_convert(x): entry = [] for i in x: entry.append(i) return entry . demo_1 = &quot;LARLTTPYC&quot; seq_convert(demo_1) . [&#39;L&#39;, &#39;A&#39;, &#39;R&#39;, &#39;L&#39;, &#39;T&#39;, &#39;T&#39;, &#39;P&#39;, &#39;Y&#39;, &#39;C&#39;] . letter_vector_train = [seq_convert(x) for x in data[&quot;Sequence&quot;]] . letter_vector_test = [seq_convert(x) for x in data_test[&quot;Sequence&quot;]] . letter_vector = np.append(letter_vector_train,letter_vector_test,axis = 0) print(len(letter_vector)) print(letter_vector) . 38 [[&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;N&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;]] . from sklearn.preprocessing import OneHotEncoder . Fitting the one hot encoder . enc = OneHotEncoder() enc.fit(letter_vector) . OneHotEncoder() . The one hod encoded vector for each of the 38 sequences in the dataset . print(enc.transform(letter_vector).toarray()) one_hot_vector = enc.transform(letter_vector).toarray() . [[0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]] . one_x_train = one_hot_vector[0:10] one_x_test = one_hot_vector[10:] print(len(one_x_train)) print(len(one_x_test)) . 10 28 . reg = LinearRegression() reg.fit(one_x_train,y_train) . LinearRegression() . Training Score . reg.score(one_x_train,y_train) . 1.0 . prediction_one = reg.predict(one_x_test) print(prediction_one) . [-1.82 -1.08 -1.5 -2.05 -1.94 -2.47 -1.82 -1.2 -1.73 -1.08 -1.62 -0.97 -1.5 -2.17 -2.7 -2.05 -2.59 -1.94 -2.47 -1.85 -1.2 -1.73 -1.62 -2.17 -2.82 -2.7 -2.59 -2.82] . Test Score . print(r2_score(y_test,prediction_one)) . 0.7308241521828207 . Pearson Correlation Co-efficient . corr, _ = pearsonr(y_test, prediction_one) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.893 . plt.scatter(range(1,29),y_test,c = &quot;blue&quot;) plt.plot(range(1,29),prediction_one , c=&quot;red&quot;) plt.xlabel(&quot;mutant number&quot;) plt.ylabel(&quot;G - value&quot;) plt.legend([&quot;predicted&quot;,&quot;y_test&quot;]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . comparision_dict= {&quot;EIIP + LR&quot;:prediction,&quot;One_Hot + LR&quot;:prediction_one} comparision_df = pd.DataFrame(comparision_dict) comparision_df . EIIP + LR One_Hot + LR . 0 -1.82 | -1.82 | . 1 -1.08 | -1.08 | . 2 -1.50 | -1.50 | . 3 -2.05 | -2.05 | . 4 -1.94 | -1.94 | . 5 -2.47 | -2.47 | . 6 -1.82 | -1.82 | . 7 -1.20 | -1.20 | . 8 -1.73 | -1.73 | . 9 -1.08 | -1.08 | . 10 -1.62 | -1.62 | . 11 -0.97 | -0.97 | . 12 -1.50 | -1.50 | . 13 -2.17 | -2.17 | . 14 -2.70 | -2.70 | . 15 -2.05 | -2.05 | . 16 -2.59 | -2.59 | . 17 -1.94 | -1.94 | . 18 -2.47 | -2.47 | . 19 -1.85 | -1.85 | . 20 -1.20 | -1.20 | . 21 -1.73 | -1.73 | . 22 -1.62 | -1.62 | . 23 -2.17 | -2.17 | . 24 -2.82 | -2.82 | . 25 -2.70 | -2.70 | . 26 -2.59 | -2.59 | . 27 -2.82 | -2.82 | . From the above results, we can quite confidently say that for a simple linear model , the numerical value assigned to each amino acid doesn&#39;t really impact the model. Thus, this model doesn&#39;t help us capture epistasis. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-1%20-%20linear_regression/2022/06/13/Introduction.html",
            "relUrl": "/part-1%20-%20linear_regression/2022/06/13/Introduction.html",
            "date": " • Jun 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About me",
          "content": "ASHWATH SREERAM . No 9, 9th Cross Street, Shastri Nagar, Adyar, Chennai - 20 · +91 9940194789 . ashwathsreeram@gmail.com . Work Experience . JANUARY 2022 – PRESENT . PROJECT INTERN, IIT HYDERABAD . I am working on projects involving utilization of different Machine Learning models to solve problems in the domain of biology. Currently, I am exploring avenues to utilize the primary sequence of proteins in order to predict vital bio-chemical characteristics. The project is headed by Dr. Sumohana Channappayya, Associate Professor, Department of Electrical Engineering, IIT Hyderabad. . MAY 2021 – JULY 2021 . INTERN, EJYLE TECHNOLOGIES (P) LTD. IND . The internship experience at Ejyle gave me an opportunity to build on my practical experience with programming using Python and R. . Projects: . Data collection from variety of PDF sources using Python packages like pdfminer. | Data Comparison between CSV files in python. | Displaying tabular data using R markdown. | Working on backend for a machine learning platform using Pandas. Specifically, using Pandas to program the data modification/engineering module which is linked to the UI of the platform. | AUGUST 2020 – APRIL 2021 . DATA ANALYST, HEALTHSOFT MIDDLE EAST. UAE . During my tenure at Healthsoft Middle East, I worked with a team involved in conducting Medical Audits for the largest government payer in the UAE. . Chief Responsibilities: . Trend Analysis of provider data. | Sampling methodologies for sampling of claims data to detect non-compliance. | Created Visual Dashboards using the data obtained from the medical audit. Dashboards created for each individual provider, containing data which will be used by payer for evaluating the compliance of the provider. | Co-ordinating with the payer and audit team in order to submit timely Audit Status Reports, Provider-Wise Audit Report and Audit Recovery report. For this purpose, I served as a communication bridge between the Chief Auditor and the payer team. | Extensive troubleshooting involving consolidation of audit findings from multiple auditors and issues relating to data duplication and data errors. | Education . JULY 2020 MASTER OF SCIENCE IN BIOLOGY, UNIVERSITY OF ALABAMA AT BIRMINGHAM . GPA: 3.7 . MAY 2019 BACHELOR OF TECHNOLOGY IN INDUSTRIAL BIOTECHNOLOGY, ANNA UNIVERSITY . GPA: 7.13 . SKILLS . Python Programming | Data Science Essentials: Numpy, Pandas, Scikit Learn, Tensorflow, etc. | Deep Learning | Git version control | Data Analytics and Visualisation using Python and Excel | Project in Data Science and AI . Project Title: Video Quality Assessment using Vision Transformers . Project was a part of the Artificial Intelligence and Emerging Technologies course conducted at IIT Hyderabad. We utilized a pre-trained vision transformer model - trained on Image Net 1K dataset – to extract features from videos to predict video quality. The extracted features were passed through trained Support Vector Regression model for predicting Mos (Mean Opinion Score) score, which is used as a metric for assessing video quality. . Other Certifications . Financial Acumen for Non-Financial Managers - University of Pennsylvania (Online ) | The Economics of Health Care Delivery - University of Pennsylvania (Online ) | Python Certificate Program - Besant Technologies (In-person ) | AI and Emerging Technologies - IIT (Indian Institute of Technology) Hyderabad (Hybrid ) | .",
          "url": "https://crimsonflash27.github.io/blog-101/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://crimsonflash27.github.io/blog-101/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}