{
  
    
        "post0": {
            "title": "ML Models for Protein Studies: Regularised Regression",
            "content": "In my previous post, we learnt that a simple closed form regression is not the way to go when it comes to epistatic studies. In this post, i will be trying some regularised regression models, along with testing the consistency of the models across many iterations. The hope is to see which of these models is more consistent across different mixes and matches of our dataset. . import pandas as pd import numpy as np import matplotlib.pyplot as plt . data = pd.read_csv(&quot;ee_train.csv&quot;) data_test = pd.read_csv(&quot;ee_test.csv&quot;) data.head() . We are still going to be continuing with EIIP. We will move on to other physiochemical properties in my next post. . dict_eiip = {&#39;A&#39;: 0.0373, &#39;R&#39;: 0.0959, &#39;N&#39;: 0.0036, &#39;D&#39;: 0.1263, &#39;C&#39;: 0.0829, &#39;Q&#39;: 0.0761, &#39;E&#39;: 0.0058, &#39;G&#39;: 0.005, &#39;H&#39;: 0.0242, &#39;I&#39;: 0, &#39;L&#39;: 0, &#39;K&#39;: 0.0371, &#39;M&#39;: 0.0823, &#39;F&#39;: 0.0946, &#39;P&#39;: 0.0198, &#39;S&#39;: 0.0829, &#39;T&#39;: 0.0941, &#39;W&#39;: 0.0548, &#39;Y&#39;: 0.0516, &#39;V&#39;: 0.0057} . def num_convert(x): return [dict_eiip[i] for i in x] . x_train = [num_convert(x) for x in data[&quot;Sequence&quot;]] x_test = [num_convert(x) for x in data_test[&quot;Sequence&quot;]] . y_train = data[&quot;G&quot;] y_test = data_test[&quot;G&quot;] . x = np.append(x_train,x_test,axis = 0) y = np.append(y_train,y_test) . print(&quot;Total number of x samples: {}&quot;.format(len(x))) print(&quot;Total number of y samples: {}&quot;.format(len(y))) . Total number of x samples: 38 Total number of y samples: 38 . Regression Model . from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split . Now, what happens if we combine the training and test dataset to do a random train-test split for testing a simple linear regressor? . x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 40) . reg = LinearRegression() reg.fit(x_train,y_train) . LinearRegression() . prediction_0 = reg.predict(x_test) . R2 Score for random Training Set . reg.score(x_train,y_train) #The training score seems to be a little too high . 0.9578526758602008 . R2 score for random Test Set . print(r2_score(y_test,prediction_0)) . 0.7155946136422272 . from scipy.stats import pearsonr corr, _ = pearsonr(y_test, prediction_0) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.868 . print(reg.score(x_test,y_test)) . 0.7155946136422272 . Regression with Iterations . Creating 100 different iterations of training and test sets from the 38 entry dataset . Training and testing the Regression Model on each of them , while recording training and test scores . list_train_error = [] list_test_error = [] list_test_pr = [] for i in range(0,100): #The random state is varying x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) reg = LinearRegression() reg.fit(x_train,y_train) prediction = reg.predict(x_test) list_train_error.append(reg.score(x_train,y_train)) list_test_error.append(reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction) list_test_pr.append(corr) . print(len(list_train_error)) print(len(list_test_error)) print(len(list_test_pr)) . 100 100 100 . list_test_error[41] . -2.9262063917582427e+26 . plt.plot(range(0,100),list_train_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson Score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Ridge Regression . Ridge Regression is a regularized regression model. To put in simply: The normal method of minimizing error will lead to overfitting and the weights of the regression model will become large. Hence, we will add the term &quot;|w|^2*alpha/2&quot; - where w is the weight vector and alpha is the hyper parameter- to minimize the weights while we minimize error. . parameters = {&#39;alpha&#39;:[0.01,0.02,0.03,0.04,0.05,0.1,0.5,1,1.5,10]} . 5 fold is the default. Scoring should be done using r2 scores . from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.linear_model import Ridge from sklearn.metrics import r2_score ridge_reg = Ridge() model = GridSearchCV(ridge_reg,param_grid = parameters, scoring = &#39;r2&#39;,verbose = 3) . model.fit(x,y) . Fitting 5 folds for each of 10 candidates, totalling 50 fits [CV 1/5] END .......................alpha=0.01;, score=-0.151 total time= 0.0s [CV 2/5] END ........................alpha=0.01;, score=0.923 total time= 0.0s [CV 3/5] END ........................alpha=0.01;, score=0.758 total time= 0.0s [CV 4/5] END ........................alpha=0.01;, score=0.888 total time= 0.0s [CV 5/5] END .......................alpha=0.01;, score=-0.020 total time= 0.0s [CV 1/5] END .......................alpha=0.02;, score=-0.291 total time= 0.0s [CV 2/5] END ........................alpha=0.02;, score=0.896 total time= 0.0s [CV 3/5] END ........................alpha=0.02;, score=0.700 total time= 0.0s [CV 4/5] END ........................alpha=0.02;, score=0.861 total time= 0.0s [CV 5/5] END .......................alpha=0.02;, score=-0.299 total time= 0.0s [CV 1/5] END .......................alpha=0.03;, score=-0.519 total time= 0.0s [CV 2/5] END ........................alpha=0.03;, score=0.861 total time= 0.0s [CV 3/5] END ........................alpha=0.03;, score=0.648 total time= 0.0s [CV 4/5] END ........................alpha=0.03;, score=0.818 total time= 0.0s [CV 5/5] END .......................alpha=0.03;, score=-0.549 total time= 0.0s [CV 1/5] END .......................alpha=0.04;, score=-0.773 total time= 0.0s [CV 2/5] END ........................alpha=0.04;, score=0.823 total time= 0.0s [CV 3/5] END ........................alpha=0.04;, score=0.601 total time= 0.0s [CV 4/5] END ........................alpha=0.04;, score=0.770 total time= 0.0s [CV 5/5] END .......................alpha=0.04;, score=-0.773 total time= 0.0s [CV 1/5] END .......................alpha=0.05;, score=-1.028 total time= 0.0s [CV 2/5] END ........................alpha=0.05;, score=0.785 total time= 0.0s [CV 3/5] END ........................alpha=0.05;, score=0.558 total time= 0.0s [CV 4/5] END ........................alpha=0.05;, score=0.721 total time= 0.0s [CV 5/5] END .......................alpha=0.05;, score=-0.972 total time= 0.0s [CV 1/5] END ........................alpha=0.1;, score=-2.110 total time= 0.0s [CV 2/5] END .........................alpha=0.1;, score=0.615 total time= 0.0s [CV 3/5] END .........................alpha=0.1;, score=0.395 total time= 0.0s [CV 4/5] END .........................alpha=0.1;, score=0.503 total time= 0.0s [CV 5/5] END ........................alpha=0.1;, score=-1.700 total time= 0.0s [CV 1/5] END ........................alpha=0.5;, score=-4.792 total time= 0.0s [CV 2/5] END .........................alpha=0.5;, score=0.144 total time= 0.0s [CV 3/5] END ........................alpha=0.5;, score=-0.000 total time= 0.0s [CV 4/5] END ........................alpha=0.5;, score=-0.102 total time= 0.0s [CV 5/5] END ........................alpha=0.5;, score=-3.221 total time= 0.0s [CV 1/5] END ..........................alpha=1;, score=-5.454 total time= 0.0s [CV 2/5] END ...........................alpha=1;, score=0.017 total time= 0.0s [CV 3/5] END ..........................alpha=1;, score=-0.103 total time= 0.0s [CV 4/5] END ..........................alpha=1;, score=-0.264 total time= 0.0s [CV 5/5] END ..........................alpha=1;, score=-3.568 total time= 0.0s [CV 1/5] END ........................alpha=1.5;, score=-5.705 total time= 0.0s [CV 2/5] END ........................alpha=1.5;, score=-0.032 total time= 0.0s [CV 3/5] END ........................alpha=1.5;, score=-0.142 total time= 0.0s [CV 4/5] END ........................alpha=1.5;, score=-0.327 total time= 0.0s [CV 5/5] END ........................alpha=1.5;, score=-3.697 total time= 0.0s [CV 1/5] END .........................alpha=10;, score=-6.173 total time= 0.0s [CV 2/5] END .........................alpha=10;, score=-0.126 total time= 0.0s [CV 3/5] END .........................alpha=10;, score=-0.217 total time= 0.0s [CV 4/5] END .........................alpha=10;, score=-0.445 total time= 0.0s [CV 5/5] END .........................alpha=10;, score=-3.936 total time= 0.0s . GridSearchCV(estimator=Ridge(), param_grid={&#39;alpha&#39;: [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.5, 1, 1.5, 10]}, scoring=&#39;r2&#39;, verbose=3) . print(model.best_params_) print(model.best_score_) . {&#39;alpha&#39;: 0.01} 0.4797289871671168 . Testing the model . x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = 40) . ridge_reg = Ridge(alpha = 0.01) . ridge_reg.fit(x_train,y_train) . Ridge(alpha=0.01) . print(ridge_reg.score(x_train,y_train)) . 0.919173242817984 . prediction = ridge_reg.predict(x_test) print(r2_score(y_test,prediction)) . 0.6304571598967417 . ridge_reg.score(x_test,y_test) . 0.6304571598967417 . list_train_error_ridge = [] list_test_error_ridge = [] list_test_pr_ridge = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) ridge_reg = Ridge(alpha = 0.01) ridge_reg.fit(x_train,y_train) prediction_ridge = ridge_reg.predict(x_test) list_train_error_ridge.append(ridge_reg.score(x_train,y_train)) list_test_error_ridge.append(ridge_reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction_ridge) list_test_pr_ridge.append(corr) . plt.plot(range(0,100),list_train_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Trying ridge with my own code . parameters = [0.01,0.02,0.03,0.04,0.05,0.1,0.5,1,1.5,10,0] . The plan is simple:&gt;&gt;Create 5 folds of the dataset . Evaluate the performance of all the models on the test set of each fold. Average out the results . Pick out the best parameter . from sklearn.model_selection import KFold . kf = KFold(n_splits= 5,shuffle = True,random_state = 40) . train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . train_sets_x[4].shape # 3 and 4 have 31 samples . (31, 9) . 0,1,2 -&gt; 30 Training samples . 3,4 -&gt; 31 Training samples . def avg(x): return sum(x)/len(x) . scores_train = {} scores_test = {} for i in parameters: n = 0 ridge_reg = Ridge(alpha = i) demo_test = [] demo_train = [] while n &lt; 5: x_train = train_sets_x[n] y_train = train_sets_y[n] x_test = test_sets_x[n] y_test = test_sets_y[n] ridge_reg.fit(x_train,y_train) demo_train.append(ridge_reg.score(x_train,y_train)) demo_test.append(ridge_reg.score(x_test,y_test)) n+=1 scores_train[i] = avg(demo_train) scores_test[i] = avg(demo_test) . scores_train . {0.01: 0.880756705051388, 0.02: 0.848759592227608, 0.03: 0.8155867798977818, 0.04: 0.7820866944371316, 0.05: 0.7493107502415729, 0.1: 0.6090778779844153, 0.5: 0.22915121741853933, 1: 0.12776726541632208, 1.5: 0.08852858687863825, 10: 0.014220384963096988, 0: 0.9270556284928301} . scores_test . {0.01: 0.7658108034364817, 0.02: 0.7138505286696081, 0.03: 0.6623057644719561, 0.04: 0.6119617120212271, 0.05: 0.5638918169833707, 0.1: 0.36629142156230543, 0.5: -0.13891567744379205, 1: -0.26985323330667554, 1.5: -0.3202372579596836, 10: -0.41525624819926377, 0: 0.8017681404264831} . From what i have obtained above - through 5 fold cross validation - we can see that 0 has the highest test and train scores;this concurs with what we obtained from gridcv. However, since 0 is just normal regression, let us go with the next best:0.01 . list_train_error_ridge = [] list_test_error_ridge = [] list_test_pr_ridge = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) ridge_reg = Ridge(alpha = 0.01) ridge_reg.fit(x_train,y_train) prediction_ridge = ridge_reg.predict(x_test) list_train_error_ridge.append(ridge_reg.score(x_train,y_train)) list_test_error_ridge.append(ridge_reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction_ridge) list_test_pr_ridge.append(corr) . plt.plot(range(0,100),list_train_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Test score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr_ridge , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . Its quite evident that the variance is less with regularized regression . Lasso Regression . from sklearn.linear_model import Lasso . parameters = [0.01,0.02,0.03,0.04,0.05,0.1,0.5,1,1.5,10] . from sklearn.model_selection import KFold kf = KFold(n_splits= 5,shuffle = True,random_state = 40) train_sets_x = [] train_sets_y = [] test_sets_x = [] test_sets_y = [] for train,test in kf.split(x): train_sets_x.append(x[train]) train_sets_y.append(y[train]) test_sets_x.append(x[test]) test_sets_y.append(y[test]) . scores_train_lasso = {} scores_test_lasso = {} for i in parameters: n = 0 lasso_reg = Lasso(alpha = i) demo_test = [] demo_train = [] while n &lt; 5: x_train = train_sets_x[n] y_train = train_sets_y[n] x_test = test_sets_x[n] y_test = test_sets_y[n] lasso_reg.fit(x_train,y_train) demo_train.append(lasso_reg.score(x_train,y_train)) demo_test.append(lasso_reg.score(x_test,y_test)) n+=1 scores_train_lasso[i] = avg(demo_train) scores_test_lasso[i] = avg(demo_test) . scores_train_lasso . {0.01: 0.5512278951745879, 0.02: 0.08042368490101719, 0.03: 0.0, 0.04: 0.0, 0.05: 0.0, 0.1: 0.0, 0.5: 0.0, 1: 0.0, 1.5: 0.0, 10: 0.0} . scores_test_lasso . {0.01: 0.3203587893250848, 0.02: -0.32792906301065744, 0.03: -0.43338493165454783, 0.04: -0.43338493165454783, 0.05: -0.43338493165454783, 0.1: -0.43338493165454783, 0.5: -0.43338493165454783, 1: -0.43338493165454783, 1.5: -0.43338493165454783, 10: -0.43338493165454783} . 0.01 seems to give the best result . list_train_error_lasso = [] list_test_error_lasso = [] list_test_pr_lasso = [] for i in range(0,100): x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.25,random_state = i) lasso_reg = Lasso(alpha = 0.01) lasso_reg.fit(x_train,y_train) prediction_lasso = lasso_reg.predict(x_test) list_train_error_lasso.append(lasso_reg.score(x_train,y_train)) list_test_error_lasso.append(lasso_reg.score(x_test,y_test)) corr, _ = pearsonr(y_test, prediction_lasso) list_test_pr_lasso.append(corr) . plt.plot(range(0,100),list_train_error_lasso , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Training score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_error_lasso , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Regression score&quot;) plt.title(&quot;Testing score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . plt.plot(range(0,100),list_test_pr_lasso , c=&quot;red&quot;) plt.xlabel(&quot;Iteration&quot;) plt.ylabel(&quot;Pearson score&quot;) plt.title(&quot;Pearson score through iterations&quot;) plt.ylim([-1,1]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . There is a good chance i may need to try with finer values of alpha for this .",
            "url": "https://crimsonflash27.github.io/blog-101/part-2%20-%20regularised_regression/2022/06/16/Regression_analysis.html",
            "relUrl": "/part-2%20-%20regularised_regression/2022/06/16/Regression_analysis.html",
            "date": " • Jun 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Linear Regression Continuation",
            "content": "Multi-Amino Acid mutants in training set . What if we incorporate the multi-mutants in the training set - can we catch the epistatic interactions? . For this, we will created a mixed dataset consisting of 19 point training dataset(switched over from the test set) and 19 point test dataset. . from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from scipy.stats import pearsonr . print(len(x_test)) print(len(x_train)) . 28 10 . new_x_train = np.append(x_train,x_test[:9],axis = 0) new_x_test = x_test[9:] . 9 samples are shifted from the test dataset into training, leading us to get 19 samples in each. . print(len(new_x_train)) print(len(new_x_test)) . 19 19 . new_y_train = np.append(y_train,y_test[:9],axis = 0) new_y_test = y_test[9:] . print(len(new_y_train)) print(len(new_y_test)) . 19 19 . reg = LinearRegression() reg.fit(new_x_train,new_y_train) . LinearRegression() . Training Score: . reg.score(new_x_train,new_y_train) . 0.9578724069495184 . Predictions: . prediction_mixed = reg.predict(new_x_test) print(prediction_mixed) . [-0.96354673 -1.44197695 -0.96881818 -1.50982586 -1.72405634 -2.26506402 -1.79190525 -2.27033547 -1.7971767 -2.33818438 -1.42970294 -0.95654417 -1.49755186 -1.5028233 -1.78490269 -2.25806146 -2.32591037 -2.33118182 -2.31890781] . Test Score: . print(r2_score(new_y_test,prediction_mixed)) . 0.733971090323416 . Pearson Correlation Co-efficient: . corr, _ = pearsonr(new_y_test, prediction_mixed) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.909 . plt.scatter(range(1,20),new_y_test,c = &quot;blue&quot;) plt.plot(range(1,20),prediction_mixed , c=&quot;red&quot;) plt.xlabel(&quot;mutant number&quot;) plt.ylabel(&quot;G - value&quot;) plt.legend([&quot;predicted&quot;,&quot;y_test&quot;]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . The r2 scores seem quite identical when compared to the unmixed dataset Linear Regression Model. But, that probably isn&#39;t the best indicator for measuring this model . Now, lets analyse the epistatic interaction prediction of this mixed model: . results = pd.read_csv(&quot;mixed_lr_result.csv&quot;) . results . Actual Additive Predictions Epistatic Prediction . 0 -0.98 | -1.08 | -0.963547 | Predicts negative | . 1 -1.73 | -1.62 | -1.441977 | Doesn’t predict positive | . 2 -0.89 | -0.97 | -0.968818 | Doesn’t predict negative | . 3 -1.88 | -1.50 | -1.509826 | Doesn’t predict positive | . 4 -1.92 | -2.17 | -1.724056 | Predicts negative | . 5 -2.15 | -2.70 | -2.265064 | Predicts negative | . 6 -1.96 | -2.05 | -1.791905 | Predicts negative | . 7 -2.41 | -2.59 | -2.270335 | Predicts negative | . 8 -1.85 | -1.94 | -1.797177 | Predicts negative | . 9 -2.37 | -2.47 | -2.338184 | Predicts negative | . 10 -1.51 | -1.85 | -1.429703 | Predicts negative | . 11 -0.92 | -1.20 | -0.956544 | Predicts negative | . 12 -1.75 | -1.73 | -1.497552 | Doesn’t predict positive | . 13 -1.74 | -1.62 | -1.502823 | Doesn’t predict positive | . 14 -2.57 | -2.17 | -1.784903 | Doesn’t predict positive | . 15 -2.09 | -2.82 | -2.258061 | Predicts negative | . 16 -2.32 | -2.70 | -2.325910 | Predicts negative | . 17 -2.73 | -2.59 | -2.331182 | Doesn’t predict positive | . 18 -2.87 | -2.82 | -2.318908 | Doesn’t predict positive | . We can see that though some negative epistatic interactions are getting predicted, none of the postive interactions are. This leads us to explore some other models which might do a better job when it comes to predicting the correct epistatic state. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-1%20-%20linear_regression/2022/06/15/LR_Continutation.html",
            "relUrl": "/part-1%20-%20linear_regression/2022/06/15/LR_Continutation.html",
            "date": " • Jun 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "ML Models for Protein Studies: Introduction",
            "content": "Acknowledgement . Before getting into the actual project, I would like to thank Jahnavi Hunsigida for being a wonderful research partner on the project; and Dr.Sumohana Channappayya of IIT Hyderabad, for giving me an opportunity to work on this project under his mentorship. . Introduction . The beauty of data science lies in the fact that it can be utilised to solve problems in a wide variety of fields. And, if you do have some domain-specific knowledge , you can find that it opens up a wide variety of options that can be taken to solve the problem at hand. . My problem lies in the field of biology, and involves dealing with protein sequences. It is a well-known fact that we have multitudes of protein sequence data available. The basics of biology will tell you that the activity of any protein is dependent not just on its amino acid sequence, but also factors like the way its backbone is designed; the way it folds in 3-D space; the way protein units bind together, etc. So, the challenge lies in being able to manipulate only the sequence data – primary structure , in biological jargon - of proteins in order to build models to solve problems in the field. . The work that follows is a continuation of an already published literature titled , “A machine learning approach for reliable prediction of amino acid interactions and its application in the directed evolution of enantioselective enzymes”, which deals with using only the sequence data to predict a biological parameter called the enantioselectivity value. . Literature Summary: . The paper discusses a methodology for predicting a property – in this case, the enantioselectivity - of the input protein based on just using its primary sequence. This methodology involves using a Digital Signal Processing Technique called innov’SAR , which is a protein structure independent method. First , the protein sequences are converted into spectrums via amino acid index encoding and Fourier Transform. These spectrums are supposed to capture epistatic interactions between the different mutants. The converted inputs are used to design a ML model , which will in turn predict e-values for each of the inputs. The overall goal is to reduce the time spent in studying the activity of newly designed proteins. . The authors have already explored and established certain models and parameters for the task at hand. My objective is to try some alternative machine learning models and data manipulations on the same problem and obtain some insights of my own. . The primary steps involved are as follows: . Converting the alphabetical amino-acid sequences into a sequence consisting of their respective amino acid indexes. . Using Fourier Transform to obtain information embedded in the converted sequences. This will serve as one of the inputs, with the other being the non-Fourier transformed sequence. . Using these as input in different Machine Learning models , hoping to capture a relationship between the inputs and the ground truth we are trying to predict- The E-value. . Epistasis: Before we go into the specifics , I want to talk about an interesting concept in science called “Epistasis”. Epistasis occurs when the mutation at a certain point in the gene is affected by a mutation at another site. As you will all see, the dataset we are dealing with consists of different mutants ( single and multi-point ) of epoxide hydrolase from Aspergillus Niger (ANEH). Hence , the model we design must be able to account for the epistatic interactions that may occur when we have multiple mutant samples. This is especially important when we want to predict the enantioselectivity unknown mutants, which may have multiple mutations at different sites. . Data Modelling: . A very vital step of any Machine Learning project involves converting the raw inputs into a form suitable for interpretation by a ML algorithm. This step is especially crucial, as the representation of the input will majorly impact the performance of the model. . Amino Acid Indexes: . The first step involves converting the alphabetical sequences of the inputs into a numerical form which is suitable for use in a machine learning model. For this purpose, we are going to use amino acid indexes. Amino Acid indexes are numerical quantities we assign to each amino acid based on the physical or chemical characteristic we are dealing with ( AAindex: Amino Acid index database ). In this project, I will be trying a variety of AA indexes and comparing their performance in each modelling scenario. . Machine Learning Models: . It should be obvious that the task at hand requires us to predict a continuous value , making this a regression problem. Keeping this is mind , we will focus on the performance of 2 ML regression models: . Linear Regression – Closed Form . | Support Vector Regression . | Dataset: . Set 1: 1 Wild-Type + 9 single point mutants of ANEH . Set 2: 28 Multi-point mutants of ANEH . The E-values for the mutants were measured in the lab. . This dataset was obtained from the supplementary section of the paper. . Basic Linear Regression . We are going to start things off by using basic linear regression to model the problem. The amino acid index we are choosing is called EIIP. . import pandas as pd import numpy as np import matplotlib.pyplot as plt . dict_eiip = {&#39;A&#39;: 0.0373, &#39;R&#39;: 0.0959, &#39;N&#39;: 0.0036, &#39;D&#39;: 0.1263, &#39;C&#39;: 0.0829, &#39;Q&#39;: 0.0761, &#39;E&#39;: 0.0058, &#39;G&#39;: 0.005, &#39;H&#39;: 0.0242, &#39;I&#39;: 0, &#39;L&#39;: 0, &#39;K&#39;: 0.0371, &#39;M&#39;: 0.0823, &#39;F&#39;: 0.0946, &#39;P&#39;: 0.0198, &#39;S&#39;: 0.0829, &#39;T&#39;: 0.0941, &#39;W&#39;: 0.0548, &#39;Y&#39;: 0.0516, &#39;V&#39;: 0.0057} . def num_convert(x): return [dict_eiip[i] for i in x] . data = pd.read_csv(&quot;ee_train.csv&quot;) . data.head() . Type Sequence EE G . 0 WT | LARLTTMLC | 4 | -0.85 | . 1 1 | FARLTTMLC | 12 | -1.50 | . 2 2 | LNRLTTMLC | 7 | -1.17 | . 3 3 | LASLTTMLC | 4 | -0.85 | . 4 4 | LARYTTMLC | 4 | -0.85 | . As we can see, the Sequence column contains the protein sequences found in the training dataset. The ground truth we are dealing with here is a converted value called Free Energy (G). . Formula: . ΔΔG‡=−RT ln (E) . Where E is the enantioselectivity . data.shape . (10, 4) . x_train = [num_convert(x) for x in data[&quot;Sequence&quot;]] . print(x_train) . [[0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0.0946, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0036, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0829, 0, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0.0516, 0.0941, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0548, 0.0941, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0057, 0.0823, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0198, 0, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0.0516, 0.0829], [0, 0.0373, 0.0959, 0, 0.0941, 0.0941, 0.0823, 0, 0.0057]] . type(x_train[0][0]) . int . y_train = data[&quot;G&quot;] print(y_train) . 0 -0.85 1 -1.50 2 -1.17 3 -0.85 4 -0.85 5 -1.50 6 -0.85 7 -1.08 8 -0.85 9 -0.97 Name: G, dtype: float64 . x_train = np.array(x_train) . print(x_train) . [[0. 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0829] [0.0946 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0036 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0829 0. 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0959 0.0516 0.0941 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0959 0. 0.0548 0.0941 0.0823 0. 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0057 0.0823 0. 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0941 0.0198 0. 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0.0516 0.0829] [0. 0.0373 0.0959 0. 0.0941 0.0941 0.0823 0. 0.0057]] . The Regression Model . Ordinary Least Square Regression . from sklearn.linear_model import LinearRegression reg = LinearRegression() . reg.fit(x_train,y_train) . LinearRegression() . demo_1 = &quot;LARLTTPYC&quot; x_trial = num_convert(demo_1) . x_trial = np.array([x_trial]) print(x_trial) . [[0. 0.0373 0.0959 0. 0.0941 0.0941 0.0198 0.0516 0.0829]] . print(reg.predict(x_trial)) . [-1.08] . Now, lets go to the test dataset . data_test = pd.read_csv(&quot;ee_test.csv&quot;) data_test.head() . Sequence G EE . 0 FNSLTTMLC | -1.68 | 16.29 | . 1 LARLTTPYC | -0.87 | 4.24 | . 2 LARLWVMLC | -1.68 | 16.29 | . 3 FNSLTTPYC | -1.84 | 21.25 | . 4 FNSLTTMLV | -1.67 | 16.02 | . data_test.shape . (28, 3) . x_test = [num_convert(x) for x in data_test[&quot;Sequence&quot;]] . x_test = np.array(x_test) . prediction = reg.predict(x_test) . print(prediction) . [-1.82 -1.08 -1.5 -2.05 -1.94 -2.47 -1.82 -1.2 -1.73 -1.08 -1.62 -0.97 -1.5 -2.17 -2.7 -2.05 -2.59 -1.94 -2.47 -1.85 -1.2 -1.73 -1.62 -2.17 -2.82 -2.7 -2.59 -2.82] . y_test = data_test[&quot;G&quot;] print(y_test) . 0 -1.68 1 -0.87 2 -1.68 3 -1.84 4 -1.67 5 -2.19 6 -1.93 7 -0.90 8 -1.30 9 -0.98 10 -1.73 11 -0.89 12 -1.88 13 -1.92 14 -2.15 15 -1.96 16 -2.41 17 -1.85 18 -2.37 19 -1.51 20 -0.92 21 -1.75 22 -1.74 23 -2.57 24 -2.09 25 -2.32 26 -2.73 27 -2.87 Name: G, dtype: float64 . from sklearn.metrics import r2_score r2_score(y_test,prediction) . 0.7308241521828205 . reg.score(x_train,y_train) . 1.0 . from scipy.stats import pearsonr corr, _ = pearsonr(y_test, prediction) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.893 . plt.scatter(range(1,29),y_test,c = &quot;blue&quot;) plt.plot(range(1,29),prediction , c=&quot;red&quot;) plt.xlabel(&quot;mutant number&quot;) plt.ylabel(&quot;G - value&quot;) plt.legend([&quot;prediction&quot;,&quot;y_test&quot;]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . From just looking at the R2 score, we can probably say that the model does relatively well for the limited number of datapoints we are dealing with. However , the problem we are dealing with is not quite as simple as getting the line of closest fit due to the problem of epistasis. . To calculate epistasis , we need to see if the e-value obtained due to multiple mutations is lower or higher than the theoretical additive value obtained by adding each individual mutation. Higher leads to positive epistasis; lower leads to negative epistasis. Hence, even if the predicted e-value is close to the actual value , there is the aspect of predicting the correct epistatic state. . Now, when it comes to the linear regression model , the predicted values for the multi-mutant test set is an exact match with the theoretical additive e-values. This observation stays the same regardless of the nature of the amino-acid index we use (This will be proved in later sections of the post, where we try other AA indexes). . Now, to verify if the linear regression model is insensitive to the exact values of the indexes, lets try a one-hot encoding model. . Trying a One-hot Model . def seq_convert(x): entry = [] for i in x: entry.append(i) return entry . demo_1 = &quot;LARLTTPYC&quot; seq_convert(demo_1) . [&#39;L&#39;, &#39;A&#39;, &#39;R&#39;, &#39;L&#39;, &#39;T&#39;, &#39;T&#39;, &#39;P&#39;, &#39;Y&#39;, &#39;C&#39;] . letter_vector_train = [seq_convert(x) for x in data[&quot;Sequence&quot;]] . letter_vector_test = [seq_convert(x) for x in data_test[&quot;Sequence&quot;]] . letter_vector = np.append(letter_vector_train,letter_vector_test,axis = 0) print(len(letter_vector)) print(letter_vector) . 38 [[&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;N&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;L&#39; &#39;A&#39; &#39;R&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;T&#39; &#39;T&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;L&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;C&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;M&#39; &#39;L&#39; &#39;V&#39;] [&#39;F&#39; &#39;N&#39; &#39;S&#39; &#39;Y&#39; &#39;W&#39; &#39;V&#39; &#39;P&#39; &#39;Y&#39; &#39;V&#39;]] . from sklearn.preprocessing import OneHotEncoder . enc = OneHotEncoder() enc.fit(letter_vector) . OneHotEncoder() . print(enc.transform(letter_vector).toarray()) one_hot_vector = enc.transform(letter_vector).toarray() . [[0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0.] [0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1.] [1. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1.]] . one_x_train = one_hot_vector[0:10] one_x_test = one_hot_vector[10:] print(len(one_x_train)) print(len(one_x_test)) . 10 28 . reg = LinearRegression() reg.fit(one_x_train,y_train) . LinearRegression() . reg.score(one_x_train,y_train) . 1.0 . prediction_one = reg.predict(one_x_test) print(prediction_one) . [-1.82 -1.08 -1.5 -2.05 -1.94 -2.47 -1.82 -1.2 -1.73 -1.08 -1.62 -0.97 -1.5 -2.17 -2.7 -2.05 -2.59 -1.94 -2.47 -1.85 -1.2 -1.73 -1.62 -2.17 -2.82 -2.7 -2.59 -2.82] . print(r2_score(y_test,prediction_one)) . 0.7308241521828207 . corr, _ = pearsonr(y_test, prediction_one) print(&#39;Pearsons correlation: %.3f&#39; % corr) . Pearsons correlation: 0.893 . plt.scatter(range(1,29),y_test,c = &quot;blue&quot;) plt.plot(range(1,29),prediction_one , c=&quot;red&quot;) plt.xlabel(&quot;mutant number&quot;) plt.ylabel(&quot;G - value&quot;) plt.legend([&quot;predicted&quot;,&quot;y_test&quot;]) plt.show . &lt;function matplotlib.pyplot.show(close=None, block=None)&gt; . comparision_dict= {&quot;EIIP + LR&quot;:prediction,&quot;One_Hot + LR&quot;:prediction_one} comparision_df = pd.DataFrame(comparision_dict) comparision_df . EIIP + LR One_Hot + LR . 0 -1.82 | -1.82 | . 1 -1.08 | -1.08 | . 2 -1.50 | -1.50 | . 3 -2.05 | -2.05 | . 4 -1.94 | -1.94 | . 5 -2.47 | -2.47 | . 6 -1.82 | -1.82 | . 7 -1.20 | -1.20 | . 8 -1.73 | -1.73 | . 9 -1.08 | -1.08 | . 10 -1.62 | -1.62 | . 11 -0.97 | -0.97 | . 12 -1.50 | -1.50 | . 13 -2.17 | -2.17 | . 14 -2.70 | -2.70 | . 15 -2.05 | -2.05 | . 16 -2.59 | -2.59 | . 17 -1.94 | -1.94 | . 18 -2.47 | -2.47 | . 19 -1.85 | -1.85 | . 20 -1.20 | -1.20 | . 21 -1.73 | -1.73 | . 22 -1.62 | -1.62 | . 23 -2.17 | -2.17 | . 24 -2.82 | -2.82 | . 25 -2.70 | -2.70 | . 26 -2.59 | -2.59 | . 27 -2.82 | -2.82 | . From the above results, we can quite confidently say that for a linear model , the numerical value assigned to each amino acid doesn&#39;t really impact the model. Thus, this model doesn&#39;t capture epistasis. .",
            "url": "https://crimsonflash27.github.io/blog-101/part-1%20-%20linear_regression/2022/06/13/Introduction.html",
            "relUrl": "/part-1%20-%20linear_regression/2022/06/13/Introduction.html",
            "date": " • Jun 13, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://crimsonflash27.github.io/blog-101/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://crimsonflash27.github.io/blog-101/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}